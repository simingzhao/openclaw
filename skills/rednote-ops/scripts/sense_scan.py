#!/usr/bin/env python3
"""
sense_scan.py â€” æ¢å­çš„è‡ªåŠ¨åŒ–Senseæ‰«æå™¨

ç‹¬ç«‹è„šæœ¬ï¼Œä¸èµ°Opusã€‚ç›´æ¥è°ƒç”¨ï¼š
  - å°çº¢ä¹¦æœç´¢ï¼ˆrednote_ops MCPï¼‰
  - Exa Search API
  - Scout workspace digestï¼ˆæœ¬åœ°æ–‡ä»¶ï¼‰

æ³¨æ„ï¼šXæ•°æ®ç”±å°é»‘ä»”çš„x-opsç‹¬å é‡‡é›†ï¼Œé€šè¿‡shared-knowledgeå…±äº«ï¼Œä¸åœ¨æ­¤å¤„é‡å¤é‡‡é›†ã€‚

ç„¶åç”¨ Gemini åšç»“æ„åŒ–åˆ†æï¼Œè¾“å‡º JSON + Markdown æŠ¥å‘Šã€‚

ç”¨æ³•ï¼š
  $VENV sense_scan.py                          # å…¨é‡æ‰«æ
  $VENV sense_scan.py --sources rednote        # åªæ‰«å°çº¢ä¹¦
  $VENV sense_scan.py --sources rednote,exa    # å°çº¢ä¹¦+Exa
  $VENV sense_scan.py --keywords "AIèµšé’±,Vibe Coding"  # è‡ªå®šä¹‰å…³é”®è¯
  $VENV sense_scan.py --skip-analysis          # åªæ‹‰æ•°æ®ï¼Œä¸è°ƒGemini
  $VENV sense_scan.py --output /path/to/out    # è‡ªå®šä¹‰è¾“å‡ºç›®å½•
"""

import argparse
import json
import os
import subprocess
import sys
import time
import uuid
from datetime import datetime
from pathlib import Path

# â”€â”€ Gemini â”€â”€
try:
    from google import genai
    from google.genai import types
    HAS_GENAI = True
except ImportError:
    HAS_GENAI = False

# â”€â”€ Requests (for MCP + X API) â”€â”€
try:
    import requests
    HAS_REQUESTS = True
except ImportError:
    HAS_REQUESTS = False

# â”€â”€ Shared Knowledge Hub â”€â”€
SHARED_KNOWLEDGE_DIR = Path(os.environ.get(
    "SHARED_KNOWLEDGE_DIR",
    os.path.expanduser("~/.openclaw/shared-knowledge"),
))
sys.path.insert(0, str(SHARED_KNOWLEDGE_DIR))
try:
    from lib.keywords import KeywordManager
    from lib.index import KnowledgeIndex
    from lib.topics import TopicTracker
    HAS_SHARED_KNOWLEDGE = True
except ImportError:
    HAS_SHARED_KNOWLEDGE = False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Config
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

MODEL_PRIMARY = os.environ.get("SENSE_MODEL", "gemini-3.1-pro-preview")
MODEL_FALLBACK = "gemini-3-flash-preview"

MCP_URL = os.environ.get("REDNOTE_MCP_URL", "http://localhost:18060/mcp")

WORKSPACE = Path(os.environ.get(
    "REDNOTE_WORKSPACE",
    os.path.expanduser("~/.openclaw/workspace-rednote-ops"),
))

SCOUT_WORKSPACE = Path(os.environ.get(
    "SCOUT_WORKSPACE",
    os.path.expanduser("~/.openclaw/workspace"),
))

EXA_SCRIPT = Path(os.environ.get(
    "EXA_SCRIPT",
    os.path.expanduser("~/Desktop/openclaw/skills/exa-search/scripts/exa_search.py"),
))

# X scanning removed â€” now handled by x-ops skill via shared-knowledge

# é»˜è®¤æœç´¢å…³é”®è¯ â€” æŒ‰ strategy.json topics å¯¹é½
DEFAULT_KEYWORDS_REDNOTE = [
    "AIå‰¯ä¸š", "AIèµšé’±", "AIä¸€äººå…¬å¸", "è¶…çº§ä¸ªä½“",
    "Vibe Coding", "Vibe Marketing", "Claudeèµšé’±",
    "AIè·¨å¢ƒç”µå•†", "AIè‡ªåŠ¨åŒ–", "Cursoræ•™ç¨‹",
]

DEFAULT_KEYWORDS_EXA = [
    "AI side hustle 2026",
    "vibe coding making money",
    "AI solopreneur",
    "Claude AI monetization",
    "AI one person company",
]

# DEFAULT_KEYWORDS_X removed â€” x-ops handles X keywords via shared-knowledge

TODAY = datetime.now().strftime("%Y-%m-%d")
TODAY_DISPLAY = datetime.now().strftime("%m.%d")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MCP å°çº¢ä¹¦æœç´¢
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_mcp_session_id = None


def _mcp_init():
    global _mcp_session_id
    if _mcp_session_id is not None:
        return
    if not HAS_REQUESTS:
        print("âš ï¸ requests æœªå®‰è£…ï¼Œè·³è¿‡å°çº¢ä¹¦æœç´¢", file=sys.stderr)
        return
    payload = {
        "jsonrpc": "2.0",
        "method": "initialize",
        "params": {
            "protocolVersion": "2024-11-05",
            "capabilities": {},
            "clientInfo": {"name": "sense-scan", "version": "1.0.0"},
        },
        "id": "init-" + str(uuid.uuid4()),
    }
    try:
        resp = requests.post(MCP_URL, json=payload, timeout=15)
        resp.raise_for_status()
        _mcp_session_id = resp.headers.get("Mcp-Session-Id", "")
        headers = {"Content-Type": "application/json"}
        if _mcp_session_id:
            headers["Mcp-Session-Id"] = _mcp_session_id
        requests.post(MCP_URL, json={
            "jsonrpc": "2.0",
            "method": "notifications/initialized",
        }, headers=headers, timeout=5)
        time.sleep(0.3)
    except requests.ConnectionError:
        print(f"âš ï¸ MCP ä¸å¯ç”¨ ({MCP_URL})ï¼Œè·³è¿‡å°çº¢ä¹¦æœç´¢", file=sys.stderr)
        _mcp_session_id = "__unavailable__"
    except Exception as e:
        print(f"âš ï¸ MCP åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œè·³è¿‡å°çº¢ä¹¦æœç´¢", file=sys.stderr)
        _mcp_session_id = "__unavailable__"


def _mcp_call(method: str, params: dict | None = None, timeout: int = 60) -> dict | None:
    _mcp_init()
    if _mcp_session_id == "__unavailable__":
        return None
    payload = {
        "jsonrpc": "2.0",
        "method": "tools/call",
        "params": {"name": method, "arguments": params or {}},
        "id": str(uuid.uuid4()),
    }
    headers = {"Content-Type": "application/json"}
    if _mcp_session_id:
        headers["Mcp-Session-Id"] = _mcp_session_id
    try:
        resp = requests.post(MCP_URL, json=payload, headers=headers, timeout=timeout)
        resp.raise_for_status()
        # MCPè¿”å›204è¡¨ç¤ºè¶…æ—¶/æ— å†…å®¹
        if resp.status_code == 204 or not resp.content:
            print(f"âš ï¸ MCPè¿”å›ç©º ({method}): status={resp.status_code}", file=sys.stderr)
            return None
        data = resp.json()
        if "error" in data:
            return None
        return data.get("result", data)
    except requests.Timeout:
        print(f"âš ï¸ MCPè¶…æ—¶ ({method}): {timeout}s", file=sys.stderr)
        return None
    except Exception as e:
        print(f"âš ï¸ MCPè°ƒç”¨å¤±è´¥ ({method}): {e}", file=sys.stderr)
        return None


def scan_rednote(keywords: list[str]) -> list[dict]:
    """æœç´¢å°çº¢ä¹¦ï¼Œè¿”å›ç»“æ„åŒ–ç»“æœåˆ—è¡¨ã€‚æœç´¢å¤±è´¥æ—¶è‡ªåŠ¨fallbackåˆ°æ¨èæµã€‚"""
    print(f"\nğŸ”´ å°çº¢ä¹¦æ‰«æ â€” {len(keywords)} ä¸ªå…³é”®è¯", file=sys.stderr)
    all_results = []
    search_ok = False

    for i, kw in enumerate(keywords):
        print(f"  ğŸ” æœç´¢: {kw}", file=sys.stderr)
        # æœç´¢ç”¨15ç§’çŸ­è¶…æ—¶ï¼Œå¿«é€Ÿå¤±è´¥
        result = _mcp_call("search_feeds", {
            "keyword": kw,
            "filters": {"sort_by": "æœ€å¤šç‚¹èµ", "note_type": "å›¾æ–‡"},
        }, timeout=20)
        if result:
            items = _extract_mcp_items(result, kw)
            all_results.extend(items)
            print(f"    âœ… æ‰¾åˆ° {len(items)} æ¡", file=sys.stderr)
            search_ok = True
        else:
            print(f"    âŒ æ— ç»“æœï¼ˆå¯èƒ½è¢«åçˆ¬ï¼‰", file=sys.stderr)
            # ç¬¬ä¸€ä¸ªå…³é”®è¯å°±å¤±è´¥ï¼Œè¯´æ˜æœç´¢æ•´ä½“æŒ‚äº†ï¼Œè·³è¿‡åç»­å…³é”®è¯
            if i == 0 and not search_ok:
                print(f"  âš ï¸ æœç´¢ä¼¼ä¹ä¸å¯ç”¨ï¼Œè·³è¿‡å‰©ä½™å…³é”®è¯ï¼Œå°è¯•æ¨èæµfallback", file=sys.stderr)
                break

        time.sleep(1)  # é¿å…MCPé€Ÿç‡é™åˆ¶

    # æœç´¢å…¨æŒ‚æ—¶ï¼Œfallbackåˆ°æ¨èæµ
    if not all_results:
        print(f"  ğŸ”„ æœç´¢æ— ç»“æœï¼Œfallbackåˆ°æ¨èæµ...", file=sys.stderr)
        feeds_result = _mcp_call("list_feeds", timeout=30)
        if feeds_result:
            items = _extract_mcp_items(feeds_result, "æ¨èæµ")
            all_results.extend(items)
            print(f"    âœ… æ¨èæµè·å– {len(items)} æ¡", file=sys.stderr)
        else:
            print(f"    âŒ æ¨èæµä¹Ÿå¤±è´¥äº†", file=sys.stderr)

    return all_results


def _extract_mcp_items(result: dict, keyword: str) -> list[dict]:
    """ä» MCP search_feeds è¿”å›ä¸­æå–ç»“æ„åŒ–æ•°æ®ã€‚
    
    MCP è¿”å› content[].text æ˜¯ä¸€ä¸ªå¤§ JSON å­—ç¬¦ä¸²ï¼ˆfeeds åˆ—è¡¨ï¼‰ã€‚
    æˆ‘ä»¬æŠŠå®ƒè§£ææˆç²¾ç®€ç»“æ„ï¼Œåªä¿ç•™ Gemini éœ€è¦çš„å­—æ®µï¼Œå¤§å¹…å‡å°‘ tokenã€‚
    """
    items = []
    content_list = result.get("content", [])
    for c in content_list:
        text = c.get("text", "")
        if not text:
            continue
        # å°è¯•è§£æä¸º JSON
        try:
            data = json.loads(text) if isinstance(text, str) else text
        except (json.JSONDecodeError, TypeError):
            # ä¸æ˜¯JSONï¼Œç›´æ¥å­˜åŸæ–‡ï¼ˆæˆªæ–­ï¼‰
            items.append({
                "source": "rednote",
                "keyword": keyword,
                "raw_text": text[:2000],
            })
            continue

        # æå– feeds åˆ—è¡¨
        feeds = data.get("feeds", []) if isinstance(data, dict) else (data if isinstance(data, list) else [])
        parsed_feeds = []
        for feed in feeds:
            card = feed.get("noteCard", {})
            interact = card.get("interactInfo", {})
            user = card.get("user", {})
            parsed_feeds.append({
                "title": card.get("displayTitle", ""),
                "author": user.get("nickname", ""),
                "likes": interact.get("likedCount", "0"),
                "collects": interact.get("collectedCount", "0"),
                "comments": interact.get("commentCount", "0"),
                "shares": interact.get("sharedCount", "0"),
                "type": card.get("type", ""),
                "feed_id": feed.get("id", ""),
            })

        if parsed_feeds:
            items.append({
                "source": "rednote",
                "keyword": keyword,
                "feeds": parsed_feeds,
            })
        else:
            # fallback: å­˜åŸæ–‡æˆªæ–­
            items.append({
                "source": "rednote",
                "keyword": keyword,
                "raw_text": text[:2000],
            })

    return items


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Exa Search
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def scan_exa(keywords: list[str]) -> list[dict]:
    """è°ƒç”¨ Exa Search è„šæœ¬ï¼Œè¿”å›ç²¾ç®€ç»“æ„åŒ–ç»“æœï¼ˆèŠ‚çœGemini tokenï¼‰ã€‚"""
    print(f"\nğŸŸ¢ Exa Search â€” {len(keywords)} ä¸ªå…³é”®è¯", file=sys.stderr)
    all_results = []

    for kw in keywords:
        print(f"  ğŸ” æœç´¢: {kw}", file=sys.stderr)
        try:
            proc = subprocess.run(
                ["python3", str(EXA_SCRIPT), kw, "--summary", kw, "--json", "-n", "5"],
                capture_output=True, text=True, timeout=45,
                env={**os.environ},
            )
            if proc.returncode == 0 and proc.stdout.strip():
                try:
                    raw = json.loads(proc.stdout)
                    # æå–ç²¾ç®€æ ¼å¼ï¼šåªè¦ title + url + å‰600å­—çš„text + summary
                    results_raw = raw.get("results", raw) if isinstance(raw, dict) else raw
                    compact = []
                    for r in (results_raw if isinstance(results_raw, list) else []):
                        title = r.get("title", "")
                        url = r.get("url", "")
                        text = (r.get("text") or r.get("summary") or "")[:600]
                        published = r.get("publishedDate", "")[:10]
                        compact.append({
                            "title": title,
                            "url": url,
                            "date": published,
                            "snippet": text,
                        })
                    if compact:
                        all_results.append({
                            "source": "exa",
                            "keyword": kw,
                            "articles": compact,
                        })
                        print(f"    âœ… {len(compact)} ç¯‡æ–‡ç« ", file=sys.stderr)
                    else:
                        print(f"    âŒ è§£æä¸ºç©º", file=sys.stderr)
                except json.JSONDecodeError:
                    # éJSON â†’ å­˜åŸå§‹æ–‡æœ¬ï¼ˆexa --summaryæ— --jsonæ—¶çš„çº¯æ–‡æœ¬è¾“å‡ºï¼‰
                    text = proc.stdout.strip()[:4000]
                    all_results.append({"source": "exa", "keyword": kw, "raw_text": text})
                    print(f"    âœ… çº¯æ–‡æœ¬è¾“å‡º", file=sys.stderr)
            else:
                err = (proc.stdout or proc.stderr).strip()
                if err:
                    print(f"    âŒ {err[:150]}", file=sys.stderr)
                else:
                    print(f"    âŒ æ— è¾“å‡º", file=sys.stderr)
        except subprocess.TimeoutExpired:
            print(f"    â° è¶…æ—¶ (45s)", file=sys.stderr)
        except Exception as e:
            print(f"    âŒ é”™è¯¯: {e}", file=sys.stderr)

    return all_results


# X API scanning removed â€” now handled by x-ops skill
# X data available via shared-knowledge: index.search(query, channel="x")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Scout Workspaceï¼ˆæœ¬åœ°æ–‡ä»¶ï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def scan_scout() -> list[dict]:
    """è¯»å–å°é»‘ä»”çš„å·¡é€»æˆæœã€‚"""
    print(f"\nğŸ¾ Scout Workspace", file=sys.stderr)
    results = []

    # X data now via shared-knowledge (x-ops writes to vector index)
    # Read latest X intel from shared-knowledge if available
    if HAS_SHARED_KNOWLEDGE:
        try:
            _sk_data = SHARED_KNOWLEDGE_DIR / "data"
            _db_path = _sk_data / "vector-index" / "knowledge.db"
            if _db_path.exists():
                from lib.index import KnowledgeIndex
                _idx = KnowledgeIndex(str(_db_path))
                x_results = _idx.search("AI trends", channel="x", top_k=10, date_from=TODAY)
                _idx.close()
                if x_results:
                    x_text = "\n\n".join(
                        f"- @{r.get('metadata',{}).get('author_username','?')}: {r['text'][:300]}"
                        for r in x_results
                    )[:8000]
                    results.append({
                        "source": "scout-x",
                        "raw_text": x_text,
                    })
                    print(f"  âœ… X data from shared-knowledge: {len(x_results)} chunks", file=sys.stderr)
                else:
                    print(f"  âš ï¸ shared-knowledgeæ— ä»Šæ—¥Xæ•°æ®", file=sys.stderr)
        except Exception as e:
            print(f"  âš ï¸ shared-knowledge Xè¯»å–å¤±è´¥: {e}", file=sys.stderr)

    # YouTube summaries
    yt_dir = SCOUT_WORKSPACE / "raw" / "youtube"
    if yt_dir.is_dir():
        yt_texts = []
        for channel_dir in sorted(yt_dir.iterdir()):
            sum_dir = channel_dir / "summaries"
            if not sum_dir.is_dir():
                continue
            for f in sorted(sum_dir.iterdir()):
                if f.name.startswith(TODAY) and f.suffix == ".md":
                    yt_texts.append(f.read_text(encoding="utf-8")[:4000])
        if yt_texts:
            combined = "\n\n---\n\n".join(yt_texts)[:8000]
            results.append({
                "source": "scout-yt",
                "raw_text": combined,
            })
            print(f"  âœ… YouTube summaries: {len(yt_texts)} ç¯‡", file=sys.stderr)
        else:
            print(f"  âš ï¸ ä»Šæ—¥æ— YouTubeæ‘˜è¦", file=sys.stderr)
    else:
        print(f"  âš ï¸ YouTubeç›®å½•ä¸å­˜åœ¨", file=sys.stderr)

    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Gemini åˆ†æ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ANALYSIS_SYSTEM_PROMPT = """ä½ æ˜¯ã€Œæ¢å­ã€â€”â€” ä¸€ä¸ªç‹¬ç«‹è¿è¥å°çº¢ä¹¦AIå†…å®¹è´¦å·çš„AIåª’ä½“äººã€‚

ä½ çš„ä»»åŠ¡æ˜¯åˆ†æå¤šæºä¿¡æ¯æ‰«æç»“æœï¼Œäº§å‡ºä¸€ä»½**ç»“æ„åŒ–SenseæŠ¥å‘Š**ï¼Œä¾›åç»­å†…å®¹å†³ç­–ä½¿ç”¨ã€‚

## ä½ çš„è´¦å·å®šä½
- å¹³å°ï¼šå°çº¢ä¹¦
- é¢†åŸŸï¼šAIèµšé’±/è¶…çº§ä¸ªä½“ â€” æ•™æ™®é€šäººæ€ä¹ˆé€šè¿‡AIæé’±
- æ ¸å¿ƒTopicsï¼šClaudeå˜ç°ã€Vibe Codingå˜ç°ã€AIä¸€äººå…¬å¸ã€Vibe Marketingã€OpenClawå®æˆ˜

## åˆ†æè¦æ±‚

### 1. è¶‹åŠ¿ä¿¡å·ï¼ˆtrendsï¼‰
ä»æ‰€æœ‰ä¿¡æºä¸­æç‚¼ 5-10 ä¸ªæœ€å¼ºä¿¡å·ï¼Œæ¯ä¸ªä¿¡å·éœ€è¦ï¼š
- signal: ä¸€å¥è¯æè¿°ï¼ˆâ‰¤30å­—ï¼‰
- strength: hot/warm/emerging
- sources: åœ¨å“ªäº›ä¿¡æºå‡ºç°è¿‡
- evidence: å…·ä½“æ•°æ®è¯æ®ï¼ˆèµ/è—/æ’­æ”¾é‡ç­‰ï¼‰
- china_feasible: ä¸­å›½å¤§é™†æ˜¯å¦å¯è¡Œï¼ˆtrue/false/partialï¼‰
- topic_match: åŒ¹é…æˆ‘ä»¬å“ªä¸ª Topicï¼ˆclaude-monetization/vibe-coding/ai-one-person-company/vibe-marketing/openclaw-practical/newï¼‰

### 2. é«˜èµå¸–åˆ†æï¼ˆtop_postsï¼‰
ä»å°çº¢ä¹¦æœç´¢ç»“æœä¸­æŒ‘å‡ºäº’åŠ¨æœ€é«˜çš„ 10 æ¡å¸–å­ï¼š
- title: æ ‡é¢˜
- likes/collects/comments: äº’åŠ¨æ•°æ®ï¼ˆå°½é‡æå–ï¼Œæ²¡æœ‰å†™0ï¼‰
- keyword: æœç´¢å…³é”®è¯
- content_type: tutorial/case_study/methodology/tool_resource/overview_opinion
- hook_analysis: æ ‡é¢˜ä¸ºä»€ä¹ˆå¸å¼•äººï¼ˆ1å¥è¯ï¼‰
- angle: å¯å€Ÿé‰´çš„è§’åº¦

### 3. é€‰é¢˜å»ºè®®ï¼ˆtopic_suggestionsï¼‰
åŸºäºä»¥ä¸Šåˆ†æï¼Œæ¨è 5 ä¸ªå…·ä½“é€‰é¢˜ï¼š
- title: å»ºè®®æ ‡é¢˜ï¼ˆâ‰¤20å­—ï¼Œç¬¦åˆå°çº¢ä¹¦é£æ ¼ï¼‰
- topic_id: åŒ¹é…çš„Topic
- content_type: tutorial/case_study/methodology/tool_resource/overview_opinion  
- reasoning: ä¸ºä»€ä¹ˆå€¼å¾—å†™ï¼ˆ1-2å¥ï¼‰
- priority: high/medium/low
- reference_material: å¯å‚è€ƒçš„ç´ ææ¥æº

### 4. é£æ ¼è§‚å¯Ÿï¼ˆstyle_observationsï¼‰
å¦‚æœåœ¨é«˜èµå¸–ä¸­è§‚å¯Ÿåˆ°è§†è§‰é£æ ¼è¶‹åŠ¿ï¼š
- observation: è§‚å¯Ÿåˆ°ä»€ä¹ˆ
- implication: å¯¹æˆ‘ä»¬çš„å¯ç¤º

### 5. å…³é”®è¯çƒ­åº¦ï¼ˆkeyword_heatmapï¼‰
æ¯ä¸ªæœç´¢å…³é”®è¯çš„çƒ­åº¦è¯„ä¼°ï¼š
- keyword: å…³é”®è¯
- heat: ğŸ”¥ğŸ”¥ğŸ”¥/ğŸ”¥ğŸ”¥/ğŸ”¥/â„ï¸
- trend: rising/stable/declining
- note: å¤‡æ³¨

## è¾“å‡ºæ ¼å¼
ä¸¥æ ¼è¾“å‡º JSONï¼Œä¸è¦è¾“å‡ºä»»ä½•é¢å¤–æ–‡æœ¬ã€‚ç»“æ„ï¼š
{
  "scan_date": "YYYY-MM-DD",
  "scan_time": "HH:MM",
  "sources_scanned": ["rednote", "exa", ...],
  "trends": [...],
  "top_posts": [...],
  "topic_suggestions": [...],
  "style_observations": [...],
  "keyword_heatmap": [...],
  "executive_summary": "3-5å¥è¯æ€»ç»“ä»Šå¤©çš„ä¿¡å·å…¨æ™¯"
}

## é‡è¦
- æ•°æ®è¦å‡†ç¡®ï¼Œä¸ç¼–é€ äº’åŠ¨æ•°å­—
- ä¸­å›½å¤§é™†å¯è¡Œæ€§ç­›é€‰æ˜¯ç¡¬æ ‡å‡† â€” ä¸å¯è¡Œçš„æ–¹å‘ç›´æ¥æ ‡æ³¨ï¼Œä¸æ¨èä¸ºé€‰é¢˜
- é€‰é¢˜æ ‡é¢˜å¿…é¡»ç¬¦åˆå°çº¢ä¹¦é£æ ¼ï¼šå…·ä½“æ•°å­—+å¯æ“ä½œ+å¥½å¥‡å¿ƒç¼ºå£
- ä¸æ¶‰åŠä»»ä½•æ”¿æ²»æ•æ„Ÿè¯é¢˜"""


def analyze_with_gemini(raw_data: list[dict]) -> dict | None:
    """ç”¨ Gemini åˆ†æåŸå§‹æ‰«ææ•°æ®ï¼Œè¿”å›ç»“æ„åŒ–æŠ¥å‘Šã€‚"""
    if not HAS_GENAI:
        print("âŒ google-genai æœªå®‰è£…ï¼Œæ— æ³•åˆ†æ", file=sys.stderr)
        return None

    # ç»„è£…ç´ ææ–‡æœ¬ï¼ˆç²¾ç®€æ ¼å¼ï¼ŒèŠ‚çœ Gemini tokenï¼‰
    material_parts = []
    for item in raw_data:
        source = item.get("source", "unknown")
        keyword = item.get("keyword", "")
        header = f"## [{source}]"
        if keyword:
            header += f" å…³é”®è¯: {keyword}"

        if "feeds" in item:
            # å°çº¢ä¹¦ç»“æ„åŒ–æ•°æ® â€” ç´§å‡‘åˆ—è¡¨æ ¼å¼
            lines = [header]
            for f in item["feeds"]:
                lines.append(
                    f"- ã€Œ{f['title']}ã€ @{f['author']} | "
                    f"èµ{f['likes']} è—{f['collects']} è¯„{f['comments']}"
                )
            material_parts.append("\n".join(lines))
        elif "articles" in item:
            # Exaç²¾ç®€æ–‡ç« æ ¼å¼
            lines = [header]
            for a in item["articles"]:
                snippet = a.get("snippet", "").replace("\n", " ")[:300]
                lines.append(f"- [{a.get('date','')}] {a.get('title','')}")
                if snippet:
                    lines.append(f"  > {snippet}")
            material_parts.append("\n".join(lines))
        elif "raw_text" in item:
            material_parts.append(f"{header}\n{item['raw_text'][:4000]}")

    combined_material = "\n\n---\n\n".join(material_parts)

    # æˆªæ–­é˜²æ­¢è¶…é•¿
    MAX_MATERIAL = 80000
    if len(combined_material) > MAX_MATERIAL:
        print(f"âš ï¸ ç´ æè¿‡é•¿ ({len(combined_material)}å­—)ï¼Œæˆªæ–­è‡³ {MAX_MATERIAL}", file=sys.stderr)
        combined_material = combined_material[:MAX_MATERIAL]

    # é™„åŠ å…³é”®è¯æå–æŒ‡ä»¤
    kw_extraction_prompt = ""
    if HAS_SHARED_KNOWLEDGE:
        _kw_path = SHARED_KNOWLEDGE_DIR / "data" / "keywords.json"
        if _kw_path.exists():
            try:
                _km_for_prompt = KeywordManager(str(_kw_path))
                existing_rn = ", ".join(_km_for_prompt.get("rednote"))
                existing_exa = ", ".join(_km_for_prompt.get("exa"))
                kw_extraction_prompt = (
                    f"\n\n## é¢å¤–ä»»åŠ¡ï¼šæå–æ–°å…´å…³é”®è¯\n"
                    f"å½“å‰å°çº¢ä¹¦è¯åº“: {existing_rn}\n"
                    f"å½“å‰Exaè¯åº“: {existing_exa}\n\n"
                    f"è¯·åœ¨JSONè¾“å‡ºä¸­é¢å¤–æ·»åŠ ä¸€ä¸ª \"new_keywords\" å­—æ®µï¼š\n"
                    f'{{"new_keywords": {{\n'
                    f'  "rednote": ["ä¸­æ–‡æ–°è¯1", "ä¸­æ–‡æ–°è¯2", ...],\n'
                    f'  "exa": ["english new term 1", "english new term 2", ...]\n'
                    f'}}}}\n'
                    f"è¦æ±‚ï¼š\n"
                    f"- æ¯ä¸ªæ¸ é“3-5ä¸ªæ–°è¯\n"
                    f"- å¿…é¡»æ˜¯å½“å‰è¯åº“ä¸­**æ²¡æœ‰çš„**\n"
                    f"- åœ¨æ‰«æç»“æœä¸­å‡ºç°é¢‘ç‡é«˜æˆ–å¢é•¿è¶‹åŠ¿æ˜æ˜¾\n"
                    f"- ä¸èµ›é“(è·¨å¢ƒç”µå•†/Vibe Coding/å¤–è´¸/çŒå¤´/AIèµšé’±)ç›¸å…³\n"
                    f"- å°çº¢ä¹¦ç»™ä¸­æ–‡è¯ï¼ŒExaç»™è‹±æ–‡è¯"
                )
            except Exception:
                pass

    user_prompt = (
        f"ä»Šå¤©æ—¥æœŸï¼š{TODAY}\n"
        f"æ‰«ææ—¶é—´ï¼š{datetime.now().strftime('%H:%M')} PST\n\n"
        f"ä»¥ä¸‹æ˜¯å¤šæºæ‰«æçš„åŸå§‹æ•°æ®ï¼Œè¯·åˆ†æå¹¶äº§å‡ºç»“æ„åŒ–SenseæŠ¥å‘Šï¼š\n\n"
        f"{combined_material}\n\n"
        f"{kw_extraction_prompt}\n\n"
        f"ä¸¥æ ¼æŒ‰ç³»ç»Ÿæç¤ºçš„JSONæ ¼å¼è¾“å‡ºã€‚ç¡®ä¿JSONå®Œæ•´å¯è§£æã€‚"
    )

    print(f"\nğŸ§  Gemini åˆ†æä¸­... (ç´ æ {len(combined_material)} å­—)", file=sys.stderr)

    client = genai.Client()

    for model in [MODEL_PRIMARY, MODEL_FALLBACK]:
        try:
            print(f"  ğŸ¤– å°è¯•: {model}", file=sys.stderr)
            response = client.models.generate_content(
                model=model,
                contents=user_prompt,
                config=types.GenerateContentConfig(
                    system_instruction=ANALYSIS_SYSTEM_PROMPT,
                    temperature=0.4,  # åˆ†æä»»åŠ¡ç”¨ä½æ¸©åº¦
                    max_output_tokens=16384,
                ),
            )
            if response and response.text:
                print(f"  âœ… åˆ†æå®Œæˆ ({model})", file=sys.stderr)
                return _parse_analysis(response.text, model)
        except Exception as e:
            if "503" in str(e) or "UNAVAILABLE" in str(e) or "429" in str(e):
                print(f"  âš ï¸ {model} ä¸å¯ç”¨: {e}", file=sys.stderr)
                continue
            print(f"  âŒ {model} é”™è¯¯: {e}", file=sys.stderr)
            continue

    print("âŒ æ‰€æœ‰æ¨¡å‹éƒ½ä¸å¯ç”¨", file=sys.stderr)
    return None


def _parse_analysis(text: str, model: str) -> dict | None:
    """ä» Gemini å“åº”ä¸­è§£æ JSONã€‚"""
    raw = text.strip()
    if "```json" in raw:
        raw = raw.split("```json", 1)[1]
        raw = raw.split("```", 1)[0]
    elif "```" in raw:
        raw = raw.split("```", 1)[1]
        raw = raw.split("```", 1)[0]
    raw = raw.strip()

    try:
        data = json.loads(raw)
        data["_model_used"] = model
        return data
    except json.JSONDecodeError as e:
        print(f"âŒ JSONè§£æå¤±è´¥: {e}", file=sys.stderr)
        print(f"  åŸå§‹ç‰‡æ®µ: {text[:500]}", file=sys.stderr)
        return None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# è¾“å‡º
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def save_outputs(raw_data: list[dict], analysis: dict | None, output_dir: Path):
    """ä¿å­˜æ‰«æç»“æœï¼šraw JSON + åˆ†æ JSON + Markdown æŠ¥å‘Šã€‚"""
    output_dir.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime("%H%M")

    # 1. åŸå§‹æ•°æ®
    raw_path = output_dir / f"{TODAY}_{timestamp}_raw.json"
    with open(raw_path, "w", encoding="utf-8") as f:
        json.dump(raw_data, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ“ åŸå§‹æ•°æ®: {raw_path}", file=sys.stderr)

    # 2. åˆ†æç»“æœ JSON
    if analysis:
        analysis_path = output_dir / f"{TODAY}_{timestamp}_analysis.json"
        with open(analysis_path, "w", encoding="utf-8") as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“ åˆ†æç»“æœ: {analysis_path}", file=sys.stderr)

        # 3. Markdown æŠ¥å‘Šï¼ˆä¾›äººé˜…è¯» + å†™å…¥ trendsï¼‰
        md_path = output_dir / f"{TODAY}_{timestamp}_report.md"
        md_content = _render_markdown(analysis)
        with open(md_path, "w", encoding="utf-8") as f:
            f.write(md_content)
        print(f"ğŸ“ Markdown: {md_path}", file=sys.stderr)

        # 4. åŒæ­¥åˆ° knowledge/trends/
        trends_dir = WORKSPACE / "knowledge" / "trends"
        trends_dir.mkdir(parents=True, exist_ok=True)
        trends_path = trends_dir / f"{TODAY}.md"
        # å¦‚æœå·²å­˜åœ¨ï¼Œè¿½åŠ ï¼›å¦åˆ™æ–°å»º
        mode = "a" if trends_path.exists() else "w"
        with open(trends_path, mode, encoding="utf-8") as f:
            if mode == "a":
                f.write(f"\n\n---\n\n# è¡¥å……æ‰«æ ({timestamp})\n\n")
            f.write(md_content)
        print(f"ğŸ“ è¶‹åŠ¿è®°å½•: {trends_path}", file=sys.stderr)

    # 5. æœ€æ–°åˆ†æçš„å¿«æ·å¼•ç”¨ï¼ˆlatest.jsonï¼‰
    if analysis:
        latest_path = output_dir / "latest.json"
        with open(latest_path, "w", encoding="utf-8") as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“ Latest: {latest_path}", file=sys.stderr)

    # 6. åŒæ­¥åˆ° shared-knowledge/data/raw/ (æŒ‰channelåˆ†ç›®å½•)
    _sync_raw_to_shared_knowledge(raw_data, analysis, timestamp)


def _sync_raw_to_shared_knowledge(raw_data: list[dict], analysis: dict | None, timestamp: str):
    """å°†rawæ•°æ®æŒ‰channelæ‹†åˆ†å†™å…¥ shared-knowledge/data/raw/"""
    sk_raw = Path(os.path.expanduser("~/.openclaw/shared-knowledge/data/raw"))
    sk_digest = Path(os.path.expanduser("~/.openclaw/shared-knowledge/data/digest"))

    for item in raw_data:
        src = item.get("source", "")
        if src == "rednote":
            keyword = item.get("keyword", "unknown")
            feeds = item.get("feeds", [])
            if not feeds:
                continue
            out_items = []
            for feed in feeds:
                out_items.append({
                    "keyword": keyword,
                    "title": feed.get("title") or feed.get("display_title", ""),
                    "author": feed.get("author", ""),
                    "likes": feed.get("likes", 0),
                    "collects": feed.get("collects", 0),
                    "comments": feed.get("comments", 0),
                    "shares": feed.get("shares", 0),
                    "type": feed.get("type", ""),
                    "feed_id": feed.get("feed_id", ""),
                    "xsec_token": feed.get("xsec_token", ""),
                    "scan_date": TODAY,
                    "scan_time": timestamp,
                })
            out_dir = sk_raw / "rednote" / TODAY
            out_dir.mkdir(parents=True, exist_ok=True)
            out_file = out_dir / f"{TODAY}_{timestamp}_raw.json"
            # è¿½åŠ æ¨¡å¼ï¼šå¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆå¹¶
            existing = []
            if out_file.exists():
                try:
                    existing = json.loads(out_file.read_text(encoding="utf-8"))
                except Exception:
                    pass
            existing.extend(out_items)
            out_file.write_text(json.dumps(existing, ensure_ascii=False, indent=2), encoding="utf-8")

        elif src == "exa":
            keyword = item.get("keyword", "unknown")
            articles = item.get("articles", [])
            if not articles:
                continue
            out_items = []
            for article in articles:
                out_items.append({
                    "keyword": keyword,
                    "title": article.get("title", ""),
                    "url": article.get("url", ""),
                    "date": article.get("date", ""),
                    "snippet": article.get("snippet", ""),
                    "scan_date": TODAY,
                    "scan_time": timestamp,
                })
            out_dir = sk_raw / "exa" / TODAY
            out_dir.mkdir(parents=True, exist_ok=True)
            out_file = out_dir / f"{TODAY}_{timestamp}_raw.json"
            existing = []
            if out_file.exists():
                try:
                    existing = json.loads(out_file.read_text(encoding="utf-8"))
                except Exception:
                    pass
            existing.extend(out_items)
            out_file.write_text(json.dumps(existing, ensure_ascii=False, indent=2), encoding="utf-8")

    # åˆ†ææŠ¥å‘Šå†™å…¥digest/scans/
    if analysis:
        scans_dir = sk_digest / "scans"
        scans_dir.mkdir(parents=True, exist_ok=True)
        report_path = scans_dir / f"{TODAY}_{timestamp}_report.md"
        if not report_path.exists():
            report_path.write_text(_render_markdown(analysis), encoding="utf-8")

        # è¶‹åŠ¿æ—¥å¿—å†™å…¥digest/daily/
        daily_dir = sk_digest / "daily"
        daily_dir.mkdir(parents=True, exist_ok=True)
        daily_path = daily_dir / f"{TODAY}.md"
        md_content = _render_markdown(analysis)
        mode = "a" if daily_path.exists() else "w"
        with open(daily_path, mode, encoding="utf-8") as f:
            if mode == "a":
                f.write(f"\n\n---\n\n# è¡¥å……æ‰«æ ({timestamp})\n\n")
            f.write(md_content)


def _render_markdown(analysis: dict) -> str:
    """å°†åˆ†æ JSON æ¸²æŸ“ä¸ºå¯è¯» Markdownã€‚"""
    lines = []
    lines.append(f"# Sense æ‰«ææŠ¥å‘Š â€” {analysis.get('scan_date', TODAY)}")
    lines.append(f"\næ‰«ææ—¶é—´ï¼š{analysis.get('scan_time', '??:??')} PST")
    lines.append(f"ä¿¡æºï¼š{', '.join(analysis.get('sources_scanned', []))}")
    model = analysis.get('_model_used', 'unknown')
    lines.append(f"åˆ†ææ¨¡å‹ï¼š{model}")

    # Executive Summary
    summary = analysis.get("executive_summary", "")
    if summary:
        lines.append(f"\n## ğŸ“‹ æ€»ç»“\n\n{summary}")

    # Trends
    trends = analysis.get("trends", [])
    if trends:
        lines.append("\n## ğŸ”¥ è¶‹åŠ¿ä¿¡å·\n")
        lines.append("| # | ä¿¡å· | å¼ºåº¦ | æ¥æº | ä¸­å›½å¯è¡Œ | Topic |")
        lines.append("|---|------|------|------|----------|-------|")
        for i, t in enumerate(trends, 1):
            signal = t.get("signal", "?")
            strength = t.get("strength", "?")
            sources = ", ".join(t.get("sources", []))
            cf = t.get("china_feasible")
            feasible = "âœ…" if cf is True or cf == "true" or cf == True else ("âš ï¸" if cf == "partial" or cf == "partly" else "âŒ")
            topic = t.get("topic_match", "?")
            lines.append(f"| {i} | {signal} | {strength} | {sources} | {feasible} | {topic} |")
        lines.append("")
        for t in trends:
            evidence = t.get("evidence", "")
            if evidence:
                lines.append(f"- **{t.get('signal', '?')}**: {evidence}")

    # Top Posts
    top_posts = analysis.get("top_posts", [])
    if top_posts:
        lines.append("\n## ğŸ“Š é«˜èµå¸–åˆ†æ\n")
        lines.append("| # | æ ‡é¢˜ | èµ/è—/è¯„ | ç±»å‹ | é’©å­åˆ†æ |")
        lines.append("|---|------|----------|------|----------|")
        for i, p in enumerate(top_posts, 1):
            title = p.get("title", "?")[:25]
            likes = p.get("likes", 0)
            collects = p.get("collects", 0)
            comments = p.get("comments", 0)
            ct = p.get("content_type", "?")
            hook = p.get("hook_analysis", "")[:30]
            lines.append(f"| {i} | {title} | {likes}/{collects}/{comments} | {ct} | {hook} |")

    # Topic Suggestions
    suggestions = analysis.get("topic_suggestions", [])
    if suggestions:
        lines.append("\n## ğŸ’¡ é€‰é¢˜å»ºè®®\n")
        for i, s in enumerate(suggestions, 1):
            title = s.get("title", "?")
            topic = s.get("topic_id", "?")
            priority = s.get("priority", "?")
            reasoning = s.get("reasoning", "")
            ct = s.get("content_type", "?")
            lines.append(f"### {i}. ã€Œ{title}ã€")
            lines.append(f"- Topic: {topic} | ç±»å‹: {ct} | ä¼˜å…ˆçº§: {priority}")
            lines.append(f"- ç†ç”±: {reasoning}")
            ref = s.get("reference_material", "")
            if ref:
                lines.append(f"- å‚è€ƒ: {ref}")
            lines.append("")

    # Style Observations
    style_obs = analysis.get("style_observations", [])
    if style_obs:
        lines.append("\n## ğŸ¨ é£æ ¼è§‚å¯Ÿ\n")
        for obs in style_obs:
            lines.append(f"- **{obs.get('observation', '?')}** â†’ {obs.get('implication', '')}")

    # Keyword Heatmap
    heatmap = analysis.get("keyword_heatmap", [])
    if heatmap:
        lines.append("\n## ğŸŒ¡ï¸ å…³é”®è¯çƒ­åº¦\n")
        lines.append("| å…³é”®è¯ | çƒ­åº¦ | è¶‹åŠ¿ | å¤‡æ³¨ |")
        lines.append("|--------|------|------|------|")
        for kw in heatmap:
            lines.append(f"| {kw.get('keyword', '?')} | {kw.get('heat', '?')} | {kw.get('trend', '?')} | {kw.get('note', '')} |")

    lines.append(f"\n---\n*è‡ªåŠ¨ç”Ÿæˆ â€” sense_scan.py | {model}*")
    return "\n".join(lines)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Main
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Shared Knowledge å›å†™
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _sync_shared_knowledge(raw_data: list[dict], analysis: dict, km: "KeywordManager | None", sources: set[str]):
    """åˆ†æå®Œæˆåï¼Œå›å†™åˆ°å…±äº«çŸ¥è¯†åº“ï¼šå…³é”®è¯è¿›åŒ– + å‘é‡å…¥åº“ + è¯é¢˜æ›´æ–°ã€‚"""
    print(f"\nğŸ“š å›å†™ Shared Knowledge...", file=sys.stderr)
    sk_data = SHARED_KNOWLEDGE_DIR / "data"

    # â”€â”€ 1. å…³é”®è¯è¿›åŒ– â”€â”€
    new_keywords = analysis.get("new_keywords", {})
    if km and new_keywords:
        for channel_key, words in new_keywords.items():
            if isinstance(words, list) and words:
                word_dicts = [{"keyword": w, "source": f"sense:{TODAY}"} for w in words if isinstance(w, str) and w.strip()]
                if word_dicts:
                    km.evolve(channel_key, word_dicts)
                    print(f"  ğŸ”‘ {channel_key} +{len(word_dicts)} æ–°è¯: {', '.join(w['keyword'] for w in word_dicts[:5])}", file=sys.stderr)

        # è®°å½•å‘½ä¸­/æœªå‘½ä¸­
        for item in raw_data:
            keyword = item.get("keyword", "")
            if not keyword:
                continue
            source = item.get("source", "")
            channel = "rednote" if source == "rednote" else ("exa" if source == "exa" else "")
            if not channel:
                continue
            has_content = bool(item.get("feeds") or item.get("articles") or item.get("raw_text"))
            if has_content:
                km.record_hit(channel, keyword)
            else:
                km.record_miss(channel, keyword)

        km.gc()
        km.save()
        print(f"  âœ… å…³é”®è¯è¯åº“å·²æ›´æ–°", file=sys.stderr)
    elif not new_keywords:
        print(f"  âš ï¸ Geminiæœªè¿”å›new_keywordsï¼Œè·³è¿‡å…³é”®è¯è¿›åŒ–", file=sys.stderr)

    # â”€â”€ 2. å‘é‡å…¥åº“ â”€â”€
    try:
        db_path = sk_data / "vector-index" / "knowledge.db"
        index = KnowledgeIndex(str(db_path))
        chunks_added = 0

        # å°çº¢ä¹¦å¸–å­
        for item in raw_data:
            if item.get("source") != "rednote":
                continue
            keyword = item.get("keyword", "")
            for feed in item.get("feeds", []):
                title = feed.get("title", "")
                if not title:
                    continue
                text = f"{title} | ä½œè€…:{feed.get('author','')} | èµ:{feed.get('likes',0)} è—:{feed.get('collects',0)}"
                try:
                    index.add(
                        source="sense_scan",
                        channel="rednote",
                        date=TODAY,
                        title=title,
                        text=text,
                        metadata={
                            "keyword": keyword,
                            "likes": feed.get("likes", "0"),
                            "collects": feed.get("collects", "0"),
                            "feed_id": feed.get("feed_id", ""),
                        },
                        tags=["rednote-search"],
                    )
                    chunks_added += 1
                except Exception as e:
                    print(f"  âš ï¸ å…¥åº“å¤±è´¥: {e}", file=sys.stderr)

        # Exaæ–‡ç« 
        for item in raw_data:
            if item.get("source") != "exa":
                continue
            keyword = item.get("keyword", "")
            for article in item.get("articles", []):
                title = article.get("title", "")
                snippet = article.get("snippet", "")
                if not (title or snippet):
                    continue
                text = f"{title}\n{snippet}" if snippet else title
                try:
                    index.add(
                        source="sense_scan",
                        channel="exa",
                        date=article.get("date", TODAY) or TODAY,
                        title=title,
                        text=text,
                        metadata={
                            "keyword": keyword,
                            "url": article.get("url", ""),
                        },
                        tags=["exa-search"],
                    )
                    chunks_added += 1
                except Exception as e:
                    print(f"  âš ï¸ å…¥åº“å¤±è´¥: {e}", file=sys.stderr)

        index.close()
        print(f"  ğŸ“¦ å‘é‡åº“ +{chunks_added} chunks", file=sys.stderr)
    except Exception as e:
        print(f"  âŒ å‘é‡å…¥åº“å¤±è´¥: {e}", file=sys.stderr)

    # â”€â”€ 3. è¯é¢˜è¿½è¸ª â”€â”€
    try:
        topics_path = sk_data / "topics.json"
        tracker = TopicTracker(str(topics_path))

        # ä»åˆ†æç»“æœçš„trendsæå–è¯é¢˜
        for trend in analysis.get("trends", []):
            signal = trend.get("signal", "")
            if not signal:
                continue
            # ç”Ÿæˆtopic_key
            topic_key = signal.lower().replace(" ", "-").replace("ï¼Œ", "-").replace("ï¼š", "-")[:40]
            topic_key = "".join(c for c in topic_key if c.isalnum() or c == "-")

            # åˆ¤æ–­æ¸ é“
            trend_sources = trend.get("sources", [])
            for src in trend_sources:
                channel = ""
                if "rednote" in src.lower() or "å°çº¢ä¹¦" in src.lower():
                    channel = "rednote"
                elif "exa" in src.lower():
                    channel = "exa"
                elif "scout" in src.lower() or "x" in src.lower() or "twitter" in src.lower():
                    channel = "x"
                elif "youtube" in src.lower() or "yt" in src.lower():
                    channel = "youtube"
                if channel:
                    tracker.upsert(topic_key, channel, {
                        "display_name": signal,
                        "mentions": 1,
                        "last_seen": TODAY,
                        "related_track": trend.get("topic_match", ""),
                    })

        tracker.save()
        all_topics = tracker.data.get("topics", {})
        active_count = sum(1 for t in all_topics.values() if t.get("status") == "active")
        print(f"  ğŸ“‹ è¯é¢˜è¿½è¸ª: {len(all_topics)} æ€»è®¡, {active_count} active", file=sys.stderr)
    except Exception as e:
        print(f"  âŒ è¯é¢˜è¿½è¸ªå¤±è´¥: {e}", file=sys.stderr)


def main():
    parser = argparse.ArgumentParser(
        description="æ¢å­ Sense æ‰«æå™¨ â€” å¤šæºä¿¡æ¯é‡‡é›† + Geminiåˆ†æ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument(
        "--sources", default="all",
        help="æ•°æ®æºï¼ˆé€—å·åˆ†éš”ï¼‰ï¼šrednote,exa,scout,allï¼ˆé»˜è®¤allï¼‰ã€‚Xå·²ç§»è‡³x-opsã€‚",
    )
    parser.add_argument(
        "--keywords", default="",
        help="è‡ªå®šä¹‰å°çº¢ä¹¦å…³é”®è¯ï¼ˆé€—å·åˆ†éš”ï¼Œè¦†ç›–é»˜è®¤ï¼‰",
    )
    parser.add_argument(
        "--keywords-exa", default="",
        help="è‡ªå®šä¹‰Exaå…³é”®è¯ï¼ˆé€—å·åˆ†éš”ï¼‰",
    )
    # --keywords-x removed: X scanning now handled by x-ops skill
    parser.add_argument(
        "--skip-analysis", action="store_true",
        help="åªæ‹‰æ•°æ®ï¼Œä¸è°ƒGeminiåˆ†æ",
    )
    parser.add_argument(
        "--output", "-o", default="",
        help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ workspace/sense/ï¼‰",
    )
    parser.add_argument(
        "--json", action="store_true",
        help="æœ€åè¾“å‡ºåˆ†æJSONåˆ°stdout",
    )

    args = parser.parse_args()

    # è§£æ sources
    sources = set()
    if args.sources == "all":
        sources = {"rednote", "exa", "scout"}
    else:
        sources = {s.strip() for s in args.sources.split(",")}

    # è§£æå…³é”®è¯ â€” ä¼˜å…ˆä» shared-knowledge åŠ¨æ€è¯åº“è¯»å–
    _km = None
    if HAS_SHARED_KNOWLEDGE and not args.keywords and not args.keywords_exa:
        _kw_path = SHARED_KNOWLEDGE_DIR / "data" / "keywords.json"
        if _kw_path.exists():
            try:
                _km = KeywordManager(str(_kw_path))
                print(f"ğŸ“š ä½¿ç”¨ shared-knowledge åŠ¨æ€è¯åº“: {_kw_path}", file=sys.stderr)
            except Exception as e:
                print(f"âš ï¸ è¯»å–åŠ¨æ€è¯åº“å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤å…³é”®è¯", file=sys.stderr)

    if args.keywords:
        kw_rednote = [k.strip() for k in args.keywords.split(",") if k.strip()]
    elif _km:
        kw_rednote = _km.get("rednote")
        print(f"  ğŸ”´ rednote å…³é”®è¯ ({len(kw_rednote)}): {', '.join(kw_rednote[:8])}{'...' if len(kw_rednote) > 8 else ''}", file=sys.stderr)
    else:
        kw_rednote = DEFAULT_KEYWORDS_REDNOTE

    if args.keywords_exa:
        kw_exa = [k.strip() for k in args.keywords_exa.split(",") if k.strip()]
    elif _km:
        kw_exa = _km.get("exa")
        print(f"  ğŸŸ¢ exa å…³é”®è¯ ({len(kw_exa)}): {', '.join(kw_exa[:5])}{'...' if len(kw_exa) > 5 else ''}", file=sys.stderr)
    else:
        kw_exa = DEFAULT_KEYWORDS_EXA

    output_dir = Path(args.output) if args.output else WORKSPACE / "sense"

    print(f"â•â•â• æ¢å­ Sense æ‰«æ â•â•â•", file=sys.stderr)
    print(f"æ—¥æœŸ: {TODAY}", file=sys.stderr)
    print(f"ä¿¡æº: {', '.join(sorted(sources))}", file=sys.stderr)
    print(f"è¾“å‡º: {output_dir}", file=sys.stderr)

    # â”€â”€ æ•°æ®é‡‡é›† â”€â”€
    all_raw = []

    if "scout" in sources:
        all_raw.extend(scan_scout())

    if "rednote" in sources:
        all_raw.extend(scan_rednote(kw_rednote))

    if "exa" in sources:
        all_raw.extend(scan_exa(kw_exa))

    if not all_raw:
        print("\nâŒ æ‰€æœ‰ä¿¡æºå‡æ— æ•°æ®", file=sys.stderr)
        sys.exit(1)

    print(f"\nğŸ“Š æ€»è®¡é‡‡é›† {len(all_raw)} æ¡æ•°æ®", file=sys.stderr)

    # â”€â”€ Gemini åˆ†æ â”€â”€
    analysis = None
    if not args.skip_analysis:
        analysis = analyze_with_gemini(all_raw)
        if analysis:
            print(f"\nâœ… åˆ†æå®Œæˆ", file=sys.stderr)
            # stdout JSON è¾“å‡º
            if args.json:
                print(json.dumps(analysis, ensure_ascii=False, indent=2))
        else:
            print(f"\nâš ï¸ åˆ†æå¤±è´¥ï¼Œä»…ä¿å­˜åŸå§‹æ•°æ®", file=sys.stderr)

    # â”€â”€ Shared Knowledge å›å†™ â”€â”€
    if HAS_SHARED_KNOWLEDGE and analysis:
        _sync_shared_knowledge(all_raw, analysis, _km, sources)

    # â”€â”€ ä¿å­˜ â”€â”€
    save_outputs(all_raw, analysis, output_dir)

    print(f"\nâ•â•â• æ‰«æå®Œæˆ â•â•â•", file=sys.stderr)


if __name__ == "__main__":
    main()
