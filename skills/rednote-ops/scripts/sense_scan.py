#!/usr/bin/env python3
"""
sense_scan.py â€” æ¢å­çš„è‡ªåŠ¨åŒ–Senseæ‰«æå™¨

ç‹¬ç«‹è„šæœ¬ï¼Œä¸èµ°Opusã€‚ç›´æ¥è°ƒç”¨ï¼š
  - å°çº¢ä¹¦æœç´¢ï¼ˆrednote_ops MCPï¼‰
  - Exa Search API
  - X API
  - Scout workspace digestï¼ˆæœ¬åœ°æ–‡ä»¶ï¼‰

ç„¶åç”¨ Gemini åšç»“æ„åŒ–åˆ†æï¼Œè¾“å‡º JSON + Markdown æŠ¥å‘Šã€‚

ç”¨æ³•ï¼š
  $VENV sense_scan.py                          # å…¨é‡æ‰«æ
  $VENV sense_scan.py --sources rednote        # åªæ‰«å°çº¢ä¹¦
  $VENV sense_scan.py --sources rednote,exa    # å°çº¢ä¹¦+Exa
  $VENV sense_scan.py --keywords "AIèµšé’±,Vibe Coding"  # è‡ªå®šä¹‰å…³é”®è¯
  $VENV sense_scan.py --skip-analysis          # åªæ‹‰æ•°æ®ï¼Œä¸è°ƒGemini
  $VENV sense_scan.py --output /path/to/out    # è‡ªå®šä¹‰è¾“å‡ºç›®å½•
"""

import argparse
import json
import os
import subprocess
import sys
import time
import uuid
from datetime import datetime
from pathlib import Path

# â”€â”€ Gemini â”€â”€
try:
    from google import genai
    from google.genai import types
    HAS_GENAI = True
except ImportError:
    HAS_GENAI = False

# â”€â”€ Requests (for MCP + X API) â”€â”€
try:
    import requests
    HAS_REQUESTS = True
except ImportError:
    HAS_REQUESTS = False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Config
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

MODEL_PRIMARY = os.environ.get("SENSE_MODEL", "gemini-3.1-pro-preview")
MODEL_FALLBACK = "gemini-3-flash-preview"

MCP_URL = os.environ.get("REDNOTE_MCP_URL", "http://localhost:18060/mcp")

WORKSPACE = Path(os.environ.get(
    "REDNOTE_WORKSPACE",
    os.path.expanduser("~/.openclaw/workspace-rednote-ops"),
))

SCOUT_WORKSPACE = Path(os.environ.get(
    "SCOUT_WORKSPACE",
    os.path.expanduser("~/.openclaw/workspace"),
))

EXA_SCRIPT = Path(os.environ.get(
    "EXA_SCRIPT",
    os.path.expanduser("~/Desktop/openclaw/skills/exa-search/scripts/search.py"),
))

X_VENV_PYTHON = Path(os.environ.get(
    "X_VENV_PYTHON",
    os.path.expanduser("~/Desktop/openclaw/skills/x-api/.venv/bin/python3"),
))

X_SCRIPT = Path(os.environ.get(
    "X_SCRIPT",
    os.path.expanduser("~/Desktop/openclaw/skills/x-api/scripts/x_api.py"),
))

# é»˜è®¤æœç´¢å…³é”®è¯ â€” æŒ‰ strategy.json topics å¯¹é½
DEFAULT_KEYWORDS_REDNOTE = [
    "AIå‰¯ä¸š", "AIèµšé’±", "AIä¸€äººå…¬å¸", "è¶…çº§ä¸ªä½“",
    "Vibe Coding", "Vibe Marketing", "Claudeèµšé’±",
    "AIè·¨å¢ƒç”µå•†", "AIè‡ªåŠ¨åŒ–", "Cursoræ•™ç¨‹",
]

DEFAULT_KEYWORDS_EXA = [
    "AI side hustle 2026",
    "vibe coding making money",
    "AI solopreneur",
    "Claude AI monetization",
    "AI one person company",
]

DEFAULT_KEYWORDS_X = [
    "vibe coding -is:retweet lang:en",
    "AI solopreneur -is:retweet lang:en",
    "Claude Code -is:retweet lang:en",
]

TODAY = datetime.now().strftime("%Y-%m-%d")
TODAY_DISPLAY = datetime.now().strftime("%m.%d")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MCP å°çº¢ä¹¦æœç´¢
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_mcp_session_id = None


def _mcp_init():
    global _mcp_session_id
    if _mcp_session_id is not None:
        return
    if not HAS_REQUESTS:
        print("âš ï¸ requests æœªå®‰è£…ï¼Œè·³è¿‡å°çº¢ä¹¦æœç´¢", file=sys.stderr)
        return
    payload = {
        "jsonrpc": "2.0",
        "method": "initialize",
        "params": {
            "protocolVersion": "2024-11-05",
            "capabilities": {},
            "clientInfo": {"name": "sense-scan", "version": "1.0.0"},
        },
        "id": "init-" + str(uuid.uuid4()),
    }
    try:
        resp = requests.post(MCP_URL, json=payload, timeout=15)
        resp.raise_for_status()
        _mcp_session_id = resp.headers.get("Mcp-Session-Id", "")
        headers = {"Content-Type": "application/json"}
        if _mcp_session_id:
            headers["Mcp-Session-Id"] = _mcp_session_id
        requests.post(MCP_URL, json={
            "jsonrpc": "2.0",
            "method": "notifications/initialized",
        }, headers=headers, timeout=5)
        time.sleep(0.3)
    except requests.ConnectionError:
        print(f"âš ï¸ MCP ä¸å¯ç”¨ ({MCP_URL})ï¼Œè·³è¿‡å°çº¢ä¹¦æœç´¢", file=sys.stderr)
        _mcp_session_id = "__unavailable__"
    except Exception as e:
        print(f"âš ï¸ MCP åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œè·³è¿‡å°çº¢ä¹¦æœç´¢", file=sys.stderr)
        _mcp_session_id = "__unavailable__"


def _mcp_call(method: str, params: dict | None = None) -> dict | None:
    _mcp_init()
    if _mcp_session_id == "__unavailable__":
        return None
    payload = {
        "jsonrpc": "2.0",
        "method": "tools/call",
        "params": {"name": method, "arguments": params or {}},
        "id": str(uuid.uuid4()),
    }
    headers = {"Content-Type": "application/json"}
    if _mcp_session_id:
        headers["Mcp-Session-Id"] = _mcp_session_id
    try:
        resp = requests.post(MCP_URL, json=payload, headers=headers, timeout=60)
        resp.raise_for_status()
        data = resp.json()
        if "error" in data:
            return None
        return data.get("result", data)
    except Exception as e:
        print(f"âš ï¸ MCPè°ƒç”¨å¤±è´¥ ({method}): {e}", file=sys.stderr)
        return None


def scan_rednote(keywords: list[str]) -> list[dict]:
    """æœç´¢å°çº¢ä¹¦ï¼Œè¿”å›ç»“æ„åŒ–ç»“æœåˆ—è¡¨ã€‚"""
    print(f"\nğŸ”´ å°çº¢ä¹¦æ‰«æ â€” {len(keywords)} ä¸ªå…³é”®è¯", file=sys.stderr)
    all_results = []

    for kw in keywords:
        print(f"  ğŸ” æœç´¢: {kw}", file=sys.stderr)
        # æœæœ€çƒ­ï¼ˆæœ€å¤šç‚¹èµï¼‰
        result = _mcp_call("search_feeds", {
            "keyword": kw,
            "filters": {"sort_by": "æœ€å¤šç‚¹èµ", "note_type": "å›¾æ–‡"},
        })
        if result:
            # æå– MCP è¿”å›çš„å†…å®¹æ–‡æœ¬
            items = _extract_mcp_items(result, kw)
            all_results.extend(items)
            print(f"    âœ… æ‰¾åˆ° {len(items)} æ¡", file=sys.stderr)
        else:
            print(f"    âŒ æ— ç»“æœ", file=sys.stderr)

        time.sleep(1)  # é¿å…MCPé€Ÿç‡é™åˆ¶

    return all_results


def _extract_mcp_items(result: dict, keyword: str) -> list[dict]:
    """ä» MCP search_feeds è¿”å›ä¸­æå–ç»“æ„åŒ–æ•°æ®ã€‚
    
    MCP è¿”å› content[].text æ˜¯ä¸€ä¸ªå¤§ JSON å­—ç¬¦ä¸²ï¼ˆfeeds åˆ—è¡¨ï¼‰ã€‚
    æˆ‘ä»¬æŠŠå®ƒè§£ææˆç²¾ç®€ç»“æ„ï¼Œåªä¿ç•™ Gemini éœ€è¦çš„å­—æ®µï¼Œå¤§å¹…å‡å°‘ tokenã€‚
    """
    items = []
    content_list = result.get("content", [])
    for c in content_list:
        text = c.get("text", "")
        if not text:
            continue
        # å°è¯•è§£æä¸º JSON
        try:
            data = json.loads(text) if isinstance(text, str) else text
        except (json.JSONDecodeError, TypeError):
            # ä¸æ˜¯JSONï¼Œç›´æ¥å­˜åŸæ–‡ï¼ˆæˆªæ–­ï¼‰
            items.append({
                "source": "rednote",
                "keyword": keyword,
                "raw_text": text[:2000],
            })
            continue

        # æå– feeds åˆ—è¡¨
        feeds = data.get("feeds", []) if isinstance(data, dict) else (data if isinstance(data, list) else [])
        parsed_feeds = []
        for feed in feeds:
            card = feed.get("noteCard", {})
            interact = card.get("interactInfo", {})
            user = card.get("user", {})
            parsed_feeds.append({
                "title": card.get("displayTitle", ""),
                "author": user.get("nickname", ""),
                "likes": interact.get("likedCount", "0"),
                "collects": interact.get("collectedCount", "0"),
                "comments": interact.get("commentCount", "0"),
                "shares": interact.get("sharedCount", "0"),
                "type": card.get("type", ""),
                "feed_id": feed.get("id", ""),
            })

        if parsed_feeds:
            items.append({
                "source": "rednote",
                "keyword": keyword,
                "feeds": parsed_feeds,
            })
        else:
            # fallback: å­˜åŸæ–‡æˆªæ–­
            items.append({
                "source": "rednote",
                "keyword": keyword,
                "raw_text": text[:2000],
            })

    return items


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Exa Search
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def scan_exa(keywords: list[str]) -> list[dict]:
    """è°ƒç”¨ Exa Search è„šæœ¬ï¼Œè¿”å›ç²¾ç®€ç»“æ„åŒ–ç»“æœï¼ˆèŠ‚çœGemini tokenï¼‰ã€‚"""
    print(f"\nğŸŸ¢ Exa Search â€” {len(keywords)} ä¸ªå…³é”®è¯", file=sys.stderr)
    all_results = []

    for kw in keywords:
        print(f"  ğŸ” æœç´¢: {kw}", file=sys.stderr)
        try:
            proc = subprocess.run(
                ["python3", str(EXA_SCRIPT), kw, "--summary", "--json"],
                capture_output=True, text=True, timeout=45,
                env={**os.environ},
            )
            if proc.returncode == 0 and proc.stdout.strip():
                try:
                    raw = json.loads(proc.stdout)
                    # æå–ç²¾ç®€æ ¼å¼ï¼šåªè¦ title + url + å‰600å­—çš„text + summary
                    results_raw = raw.get("results", raw) if isinstance(raw, dict) else raw
                    compact = []
                    for r in (results_raw if isinstance(results_raw, list) else []):
                        title = r.get("title", "")
                        url = r.get("url", "")
                        text = (r.get("text") or r.get("summary") or "")[:600]
                        published = r.get("publishedDate", "")[:10]
                        compact.append({
                            "title": title,
                            "url": url,
                            "date": published,
                            "snippet": text,
                        })
                    if compact:
                        all_results.append({
                            "source": "exa",
                            "keyword": kw,
                            "articles": compact,
                        })
                        print(f"    âœ… {len(compact)} ç¯‡æ–‡ç« ", file=sys.stderr)
                    else:
                        print(f"    âŒ è§£æä¸ºç©º", file=sys.stderr)
                except json.JSONDecodeError:
                    # éJSON â†’ å­˜åŸå§‹æ–‡æœ¬ï¼ˆexa --summaryæ— --jsonæ—¶çš„çº¯æ–‡æœ¬è¾“å‡ºï¼‰
                    text = proc.stdout.strip()[:4000]
                    all_results.append({"source": "exa", "keyword": kw, "raw_text": text})
                    print(f"    âœ… çº¯æ–‡æœ¬è¾“å‡º", file=sys.stderr)
            else:
                err = (proc.stdout or proc.stderr).strip()
                if err:
                    print(f"    âŒ {err[:150]}", file=sys.stderr)
                else:
                    print(f"    âŒ æ— è¾“å‡º", file=sys.stderr)
        except subprocess.TimeoutExpired:
            print(f"    â° è¶…æ—¶ (45s)", file=sys.stderr)
        except Exception as e:
            print(f"    âŒ é”™è¯¯: {e}", file=sys.stderr)

    return all_results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# X API
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def scan_x(keywords: list[str]) -> list[dict]:
    """è°ƒç”¨ X API è„šæœ¬æœç´¢ã€‚"""
    print(f"\nğŸ”µ X/Twitter â€” {len(keywords)} ä¸ªå…³é”®è¯", file=sys.stderr)
    all_results = []

    for kw in keywords:
        print(f"  ğŸ” æœç´¢: {kw}", file=sys.stderr)
        try:
            proc = subprocess.run(
                [str(X_VENV_PYTHON), str(X_SCRIPT), "--human", "search", kw, "--max-results", "10"],
                capture_output=True, text=True, timeout=30,
                env={**os.environ},
            )
            output = proc.stdout.strip()
            if proc.returncode == 0 and output:
                all_results.append({
                    "source": "x",
                    "keyword": kw,
                    "raw_text": output[:5000],
                })
                print(f"    âœ… æœ‰ç»“æœ", file=sys.stderr)
            else:
                err = proc.stderr.strip()
                if err:
                    print(f"    âŒ {err[:200]}", file=sys.stderr)
                else:
                    print(f"    âŒ æ— ç»“æœ", file=sys.stderr)
        except subprocess.TimeoutExpired:
            print(f"    â° è¶…æ—¶", file=sys.stderr)
        except Exception as e:
            print(f"    âŒ é”™è¯¯: {e}", file=sys.stderr)

    return all_results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Scout Workspaceï¼ˆæœ¬åœ°æ–‡ä»¶ï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def scan_scout() -> list[dict]:
    """è¯»å–å°é»‘ä»”çš„å·¡é€»æˆæœã€‚"""
    print(f"\nğŸ¾ Scout Workspace", file=sys.stderr)
    results = []

    # X digest
    x_digest = SCOUT_WORKSPACE / "raw" / "x-posts" / f"{TODAY}_digest.md"
    if x_digest.exists():
        text = x_digest.read_text(encoding="utf-8")[:8000]
        results.append({
            "source": "scout-x",
            "raw_text": text,
        })
        print(f"  âœ… X digest: {len(text)} å­—", file=sys.stderr)
    else:
        print(f"  âš ï¸ X digest ä¸å­˜åœ¨: {x_digest}", file=sys.stderr)

    # YouTube summaries
    yt_dir = SCOUT_WORKSPACE / "raw" / "youtube"
    if yt_dir.is_dir():
        yt_texts = []
        for channel_dir in sorted(yt_dir.iterdir()):
            sum_dir = channel_dir / "summaries"
            if not sum_dir.is_dir():
                continue
            for f in sorted(sum_dir.iterdir()):
                if f.name.startswith(TODAY) and f.suffix == ".md":
                    yt_texts.append(f.read_text(encoding="utf-8")[:4000])
        if yt_texts:
            combined = "\n\n---\n\n".join(yt_texts)[:8000]
            results.append({
                "source": "scout-yt",
                "raw_text": combined,
            })
            print(f"  âœ… YouTube summaries: {len(yt_texts)} ç¯‡", file=sys.stderr)
        else:
            print(f"  âš ï¸ ä»Šæ—¥æ— YouTubeæ‘˜è¦", file=sys.stderr)
    else:
        print(f"  âš ï¸ YouTubeç›®å½•ä¸å­˜åœ¨", file=sys.stderr)

    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Gemini åˆ†æ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ANALYSIS_SYSTEM_PROMPT = """ä½ æ˜¯ã€Œæ¢å­ã€â€”â€” ä¸€ä¸ªç‹¬ç«‹è¿è¥å°çº¢ä¹¦AIå†…å®¹è´¦å·çš„AIåª’ä½“äººã€‚

ä½ çš„ä»»åŠ¡æ˜¯åˆ†æå¤šæºä¿¡æ¯æ‰«æç»“æœï¼Œäº§å‡ºä¸€ä»½**ç»“æ„åŒ–SenseæŠ¥å‘Š**ï¼Œä¾›åç»­å†…å®¹å†³ç­–ä½¿ç”¨ã€‚

## ä½ çš„è´¦å·å®šä½
- å¹³å°ï¼šå°çº¢ä¹¦
- é¢†åŸŸï¼šAIèµšé’±/è¶…çº§ä¸ªä½“ â€” æ•™æ™®é€šäººæ€ä¹ˆé€šè¿‡AIæé’±
- æ ¸å¿ƒTopicsï¼šClaudeå˜ç°ã€Vibe Codingå˜ç°ã€AIä¸€äººå…¬å¸ã€Vibe Marketingã€OpenClawå®æˆ˜

## åˆ†æè¦æ±‚

### 1. è¶‹åŠ¿ä¿¡å·ï¼ˆtrendsï¼‰
ä»æ‰€æœ‰ä¿¡æºä¸­æç‚¼ 5-10 ä¸ªæœ€å¼ºä¿¡å·ï¼Œæ¯ä¸ªä¿¡å·éœ€è¦ï¼š
- signal: ä¸€å¥è¯æè¿°ï¼ˆâ‰¤30å­—ï¼‰
- strength: hot/warm/emerging
- sources: åœ¨å“ªäº›ä¿¡æºå‡ºç°è¿‡
- evidence: å…·ä½“æ•°æ®è¯æ®ï¼ˆèµ/è—/æ’­æ”¾é‡ç­‰ï¼‰
- china_feasible: ä¸­å›½å¤§é™†æ˜¯å¦å¯è¡Œï¼ˆtrue/false/partialï¼‰
- topic_match: åŒ¹é…æˆ‘ä»¬å“ªä¸ª Topicï¼ˆclaude-monetization/vibe-coding/ai-one-person-company/vibe-marketing/openclaw-practical/newï¼‰

### 2. é«˜èµå¸–åˆ†æï¼ˆtop_postsï¼‰
ä»å°çº¢ä¹¦æœç´¢ç»“æœä¸­æŒ‘å‡ºäº’åŠ¨æœ€é«˜çš„ 10 æ¡å¸–å­ï¼š
- title: æ ‡é¢˜
- likes/collects/comments: äº’åŠ¨æ•°æ®ï¼ˆå°½é‡æå–ï¼Œæ²¡æœ‰å†™0ï¼‰
- keyword: æœç´¢å…³é”®è¯
- content_type: tutorial/case_study/methodology/tool_resource/overview_opinion
- hook_analysis: æ ‡é¢˜ä¸ºä»€ä¹ˆå¸å¼•äººï¼ˆ1å¥è¯ï¼‰
- angle: å¯å€Ÿé‰´çš„è§’åº¦

### 3. é€‰é¢˜å»ºè®®ï¼ˆtopic_suggestionsï¼‰
åŸºäºä»¥ä¸Šåˆ†æï¼Œæ¨è 5 ä¸ªå…·ä½“é€‰é¢˜ï¼š
- title: å»ºè®®æ ‡é¢˜ï¼ˆâ‰¤20å­—ï¼Œç¬¦åˆå°çº¢ä¹¦é£æ ¼ï¼‰
- topic_id: åŒ¹é…çš„Topic
- content_type: tutorial/case_study/methodology/tool_resource/overview_opinion  
- reasoning: ä¸ºä»€ä¹ˆå€¼å¾—å†™ï¼ˆ1-2å¥ï¼‰
- priority: high/medium/low
- reference_material: å¯å‚è€ƒçš„ç´ ææ¥æº

### 4. é£æ ¼è§‚å¯Ÿï¼ˆstyle_observationsï¼‰
å¦‚æœåœ¨é«˜èµå¸–ä¸­è§‚å¯Ÿåˆ°è§†è§‰é£æ ¼è¶‹åŠ¿ï¼š
- observation: è§‚å¯Ÿåˆ°ä»€ä¹ˆ
- implication: å¯¹æˆ‘ä»¬çš„å¯ç¤º

### 5. å…³é”®è¯çƒ­åº¦ï¼ˆkeyword_heatmapï¼‰
æ¯ä¸ªæœç´¢å…³é”®è¯çš„çƒ­åº¦è¯„ä¼°ï¼š
- keyword: å…³é”®è¯
- heat: ğŸ”¥ğŸ”¥ğŸ”¥/ğŸ”¥ğŸ”¥/ğŸ”¥/â„ï¸
- trend: rising/stable/declining
- note: å¤‡æ³¨

## è¾“å‡ºæ ¼å¼
ä¸¥æ ¼è¾“å‡º JSONï¼Œä¸è¦è¾“å‡ºä»»ä½•é¢å¤–æ–‡æœ¬ã€‚ç»“æ„ï¼š
{
  "scan_date": "YYYY-MM-DD",
  "scan_time": "HH:MM",
  "sources_scanned": ["rednote", "exa", ...],
  "trends": [...],
  "top_posts": [...],
  "topic_suggestions": [...],
  "style_observations": [...],
  "keyword_heatmap": [...],
  "executive_summary": "3-5å¥è¯æ€»ç»“ä»Šå¤©çš„ä¿¡å·å…¨æ™¯"
}

## é‡è¦
- æ•°æ®è¦å‡†ç¡®ï¼Œä¸ç¼–é€ äº’åŠ¨æ•°å­—
- ä¸­å›½å¤§é™†å¯è¡Œæ€§ç­›é€‰æ˜¯ç¡¬æ ‡å‡† â€” ä¸å¯è¡Œçš„æ–¹å‘ç›´æ¥æ ‡æ³¨ï¼Œä¸æ¨èä¸ºé€‰é¢˜
- é€‰é¢˜æ ‡é¢˜å¿…é¡»ç¬¦åˆå°çº¢ä¹¦é£æ ¼ï¼šå…·ä½“æ•°å­—+å¯æ“ä½œ+å¥½å¥‡å¿ƒç¼ºå£
- ä¸æ¶‰åŠä»»ä½•æ”¿æ²»æ•æ„Ÿè¯é¢˜"""


def analyze_with_gemini(raw_data: list[dict]) -> dict | None:
    """ç”¨ Gemini åˆ†æåŸå§‹æ‰«ææ•°æ®ï¼Œè¿”å›ç»“æ„åŒ–æŠ¥å‘Šã€‚"""
    if not HAS_GENAI:
        print("âŒ google-genai æœªå®‰è£…ï¼Œæ— æ³•åˆ†æ", file=sys.stderr)
        return None

    # ç»„è£…ç´ ææ–‡æœ¬ï¼ˆç²¾ç®€æ ¼å¼ï¼ŒèŠ‚çœ Gemini tokenï¼‰
    material_parts = []
    for item in raw_data:
        source = item.get("source", "unknown")
        keyword = item.get("keyword", "")
        header = f"## [{source}]"
        if keyword:
            header += f" å…³é”®è¯: {keyword}"

        if "feeds" in item:
            # å°çº¢ä¹¦ç»“æ„åŒ–æ•°æ® â€” ç´§å‡‘åˆ—è¡¨æ ¼å¼
            lines = [header]
            for f in item["feeds"]:
                lines.append(
                    f"- ã€Œ{f['title']}ã€ @{f['author']} | "
                    f"èµ{f['likes']} è—{f['collects']} è¯„{f['comments']}"
                )
            material_parts.append("\n".join(lines))
        elif "articles" in item:
            # Exaç²¾ç®€æ–‡ç« æ ¼å¼
            lines = [header]
            for a in item["articles"]:
                snippet = a.get("snippet", "").replace("\n", " ")[:300]
                lines.append(f"- [{a.get('date','')}] {a.get('title','')}")
                if snippet:
                    lines.append(f"  > {snippet}")
            material_parts.append("\n".join(lines))
        elif "raw_text" in item:
            material_parts.append(f"{header}\n{item['raw_text'][:4000]}")

    combined_material = "\n\n---\n\n".join(material_parts)

    # æˆªæ–­é˜²æ­¢è¶…é•¿
    MAX_MATERIAL = 80000
    if len(combined_material) > MAX_MATERIAL:
        print(f"âš ï¸ ç´ æè¿‡é•¿ ({len(combined_material)}å­—)ï¼Œæˆªæ–­è‡³ {MAX_MATERIAL}", file=sys.stderr)
        combined_material = combined_material[:MAX_MATERIAL]

    user_prompt = (
        f"ä»Šå¤©æ—¥æœŸï¼š{TODAY}\n"
        f"æ‰«ææ—¶é—´ï¼š{datetime.now().strftime('%H:%M')} PST\n\n"
        f"ä»¥ä¸‹æ˜¯å¤šæºæ‰«æçš„åŸå§‹æ•°æ®ï¼Œè¯·åˆ†æå¹¶äº§å‡ºç»“æ„åŒ–SenseæŠ¥å‘Šï¼š\n\n"
        f"{combined_material}\n\n"
        f"ä¸¥æ ¼æŒ‰ç³»ç»Ÿæç¤ºçš„JSONæ ¼å¼è¾“å‡ºã€‚ç¡®ä¿JSONå®Œæ•´å¯è§£æã€‚"
    )

    print(f"\nğŸ§  Gemini åˆ†æä¸­... (ç´ æ {len(combined_material)} å­—)", file=sys.stderr)

    client = genai.Client()

    for model in [MODEL_PRIMARY, MODEL_FALLBACK]:
        try:
            print(f"  ğŸ¤– å°è¯•: {model}", file=sys.stderr)
            response = client.models.generate_content(
                model=model,
                contents=user_prompt,
                config=types.GenerateContentConfig(
                    system_instruction=ANALYSIS_SYSTEM_PROMPT,
                    temperature=0.4,  # åˆ†æä»»åŠ¡ç”¨ä½æ¸©åº¦
                    max_output_tokens=16384,
                ),
            )
            if response and response.text:
                print(f"  âœ… åˆ†æå®Œæˆ ({model})", file=sys.stderr)
                return _parse_analysis(response.text, model)
        except Exception as e:
            if "503" in str(e) or "UNAVAILABLE" in str(e) or "429" in str(e):
                print(f"  âš ï¸ {model} ä¸å¯ç”¨: {e}", file=sys.stderr)
                continue
            print(f"  âŒ {model} é”™è¯¯: {e}", file=sys.stderr)
            continue

    print("âŒ æ‰€æœ‰æ¨¡å‹éƒ½ä¸å¯ç”¨", file=sys.stderr)
    return None


def _parse_analysis(text: str, model: str) -> dict | None:
    """ä» Gemini å“åº”ä¸­è§£æ JSONã€‚"""
    raw = text.strip()
    if "```json" in raw:
        raw = raw.split("```json", 1)[1]
        raw = raw.split("```", 1)[0]
    elif "```" in raw:
        raw = raw.split("```", 1)[1]
        raw = raw.split("```", 1)[0]
    raw = raw.strip()

    try:
        data = json.loads(raw)
        data["_model_used"] = model
        return data
    except json.JSONDecodeError as e:
        print(f"âŒ JSONè§£æå¤±è´¥: {e}", file=sys.stderr)
        print(f"  åŸå§‹ç‰‡æ®µ: {text[:500]}", file=sys.stderr)
        return None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# è¾“å‡º
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def save_outputs(raw_data: list[dict], analysis: dict | None, output_dir: Path):
    """ä¿å­˜æ‰«æç»“æœï¼šraw JSON + åˆ†æ JSON + Markdown æŠ¥å‘Šã€‚"""
    output_dir.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime("%H%M")

    # 1. åŸå§‹æ•°æ®
    raw_path = output_dir / f"{TODAY}_{timestamp}_raw.json"
    with open(raw_path, "w", encoding="utf-8") as f:
        json.dump(raw_data, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ“ åŸå§‹æ•°æ®: {raw_path}", file=sys.stderr)

    # 2. åˆ†æç»“æœ JSON
    if analysis:
        analysis_path = output_dir / f"{TODAY}_{timestamp}_analysis.json"
        with open(analysis_path, "w", encoding="utf-8") as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“ åˆ†æç»“æœ: {analysis_path}", file=sys.stderr)

        # 3. Markdown æŠ¥å‘Šï¼ˆä¾›äººé˜…è¯» + å†™å…¥ trendsï¼‰
        md_path = output_dir / f"{TODAY}_{timestamp}_report.md"
        md_content = _render_markdown(analysis)
        with open(md_path, "w", encoding="utf-8") as f:
            f.write(md_content)
        print(f"ğŸ“ Markdown: {md_path}", file=sys.stderr)

        # 4. åŒæ­¥åˆ° knowledge/trends/
        trends_dir = WORKSPACE / "knowledge" / "trends"
        trends_dir.mkdir(parents=True, exist_ok=True)
        trends_path = trends_dir / f"{TODAY}.md"
        # å¦‚æœå·²å­˜åœ¨ï¼Œè¿½åŠ ï¼›å¦åˆ™æ–°å»º
        mode = "a" if trends_path.exists() else "w"
        with open(trends_path, mode, encoding="utf-8") as f:
            if mode == "a":
                f.write(f"\n\n---\n\n# è¡¥å……æ‰«æ ({timestamp})\n\n")
            f.write(md_content)
        print(f"ğŸ“ è¶‹åŠ¿è®°å½•: {trends_path}", file=sys.stderr)

    # 5. æœ€æ–°åˆ†æçš„å¿«æ·å¼•ç”¨ï¼ˆlatest.jsonï¼‰
    if analysis:
        latest_path = output_dir / "latest.json"
        with open(latest_path, "w", encoding="utf-8") as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“ Latest: {latest_path}", file=sys.stderr)


def _render_markdown(analysis: dict) -> str:
    """å°†åˆ†æ JSON æ¸²æŸ“ä¸ºå¯è¯» Markdownã€‚"""
    lines = []
    lines.append(f"# Sense æ‰«ææŠ¥å‘Š â€” {analysis.get('scan_date', TODAY)}")
    lines.append(f"\næ‰«ææ—¶é—´ï¼š{analysis.get('scan_time', '??:??')} PST")
    lines.append(f"ä¿¡æºï¼š{', '.join(analysis.get('sources_scanned', []))}")
    model = analysis.get('_model_used', 'unknown')
    lines.append(f"åˆ†ææ¨¡å‹ï¼š{model}")

    # Executive Summary
    summary = analysis.get("executive_summary", "")
    if summary:
        lines.append(f"\n## ğŸ“‹ æ€»ç»“\n\n{summary}")

    # Trends
    trends = analysis.get("trends", [])
    if trends:
        lines.append("\n## ğŸ”¥ è¶‹åŠ¿ä¿¡å·\n")
        lines.append("| # | ä¿¡å· | å¼ºåº¦ | æ¥æº | ä¸­å›½å¯è¡Œ | Topic |")
        lines.append("|---|------|------|------|----------|-------|")
        for i, t in enumerate(trends, 1):
            signal = t.get("signal", "?")
            strength = t.get("strength", "?")
            sources = ", ".join(t.get("sources", []))
            cf = t.get("china_feasible")
            feasible = "âœ…" if cf is True or cf == "true" or cf == True else ("âš ï¸" if cf == "partial" or cf == "partly" else "âŒ")
            topic = t.get("topic_match", "?")
            lines.append(f"| {i} | {signal} | {strength} | {sources} | {feasible} | {topic} |")
        lines.append("")
        for t in trends:
            evidence = t.get("evidence", "")
            if evidence:
                lines.append(f"- **{t.get('signal', '?')}**: {evidence}")

    # Top Posts
    top_posts = analysis.get("top_posts", [])
    if top_posts:
        lines.append("\n## ğŸ“Š é«˜èµå¸–åˆ†æ\n")
        lines.append("| # | æ ‡é¢˜ | èµ/è—/è¯„ | ç±»å‹ | é’©å­åˆ†æ |")
        lines.append("|---|------|----------|------|----------|")
        for i, p in enumerate(top_posts, 1):
            title = p.get("title", "?")[:25]
            likes = p.get("likes", 0)
            collects = p.get("collects", 0)
            comments = p.get("comments", 0)
            ct = p.get("content_type", "?")
            hook = p.get("hook_analysis", "")[:30]
            lines.append(f"| {i} | {title} | {likes}/{collects}/{comments} | {ct} | {hook} |")

    # Topic Suggestions
    suggestions = analysis.get("topic_suggestions", [])
    if suggestions:
        lines.append("\n## ğŸ’¡ é€‰é¢˜å»ºè®®\n")
        for i, s in enumerate(suggestions, 1):
            title = s.get("title", "?")
            topic = s.get("topic_id", "?")
            priority = s.get("priority", "?")
            reasoning = s.get("reasoning", "")
            ct = s.get("content_type", "?")
            lines.append(f"### {i}. ã€Œ{title}ã€")
            lines.append(f"- Topic: {topic} | ç±»å‹: {ct} | ä¼˜å…ˆçº§: {priority}")
            lines.append(f"- ç†ç”±: {reasoning}")
            ref = s.get("reference_material", "")
            if ref:
                lines.append(f"- å‚è€ƒ: {ref}")
            lines.append("")

    # Style Observations
    style_obs = analysis.get("style_observations", [])
    if style_obs:
        lines.append("\n## ğŸ¨ é£æ ¼è§‚å¯Ÿ\n")
        for obs in style_obs:
            lines.append(f"- **{obs.get('observation', '?')}** â†’ {obs.get('implication', '')}")

    # Keyword Heatmap
    heatmap = analysis.get("keyword_heatmap", [])
    if heatmap:
        lines.append("\n## ğŸŒ¡ï¸ å…³é”®è¯çƒ­åº¦\n")
        lines.append("| å…³é”®è¯ | çƒ­åº¦ | è¶‹åŠ¿ | å¤‡æ³¨ |")
        lines.append("|--------|------|------|------|")
        for kw in heatmap:
            lines.append(f"| {kw.get('keyword', '?')} | {kw.get('heat', '?')} | {kw.get('trend', '?')} | {kw.get('note', '')} |")

    lines.append(f"\n---\n*è‡ªåŠ¨ç”Ÿæˆ â€” sense_scan.py | {model}*")
    return "\n".join(lines)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Main
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    parser = argparse.ArgumentParser(
        description="æ¢å­ Sense æ‰«æå™¨ â€” å¤šæºä¿¡æ¯é‡‡é›† + Geminiåˆ†æ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument(
        "--sources", default="all",
        help="æ•°æ®æºï¼ˆé€—å·åˆ†éš”ï¼‰ï¼šrednote,exa,x,scout,allï¼ˆé»˜è®¤allï¼‰",
    )
    parser.add_argument(
        "--keywords", default="",
        help="è‡ªå®šä¹‰å°çº¢ä¹¦å…³é”®è¯ï¼ˆé€—å·åˆ†éš”ï¼Œè¦†ç›–é»˜è®¤ï¼‰",
    )
    parser.add_argument(
        "--keywords-exa", default="",
        help="è‡ªå®šä¹‰Exaå…³é”®è¯ï¼ˆé€—å·åˆ†éš”ï¼‰",
    )
    parser.add_argument(
        "--keywords-x", default="",
        help="è‡ªå®šä¹‰Xå…³é”®è¯ï¼ˆé€—å·åˆ†éš”ï¼‰",
    )
    parser.add_argument(
        "--skip-analysis", action="store_true",
        help="åªæ‹‰æ•°æ®ï¼Œä¸è°ƒGeminiåˆ†æ",
    )
    parser.add_argument(
        "--output", "-o", default="",
        help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ workspace/sense/ï¼‰",
    )
    parser.add_argument(
        "--json", action="store_true",
        help="æœ€åè¾“å‡ºåˆ†æJSONåˆ°stdout",
    )

    args = parser.parse_args()

    # è§£æ sources
    sources = set()
    if args.sources == "all":
        sources = {"rednote", "exa", "x", "scout"}
    else:
        sources = {s.strip() for s in args.sources.split(",")}

    # è§£æå…³é”®è¯
    kw_rednote = [k.strip() for k in args.keywords.split(",") if k.strip()] if args.keywords else DEFAULT_KEYWORDS_REDNOTE
    kw_exa = [k.strip() for k in args.keywords_exa.split(",") if k.strip()] if args.keywords_exa else DEFAULT_KEYWORDS_EXA
    kw_x = [k.strip() for k in args.keywords_x.split(",") if k.strip()] if args.keywords_x else DEFAULT_KEYWORDS_X

    output_dir = Path(args.output) if args.output else WORKSPACE / "sense"

    print(f"â•â•â• æ¢å­ Sense æ‰«æ â•â•â•", file=sys.stderr)
    print(f"æ—¥æœŸ: {TODAY}", file=sys.stderr)
    print(f"ä¿¡æº: {', '.join(sorted(sources))}", file=sys.stderr)
    print(f"è¾“å‡º: {output_dir}", file=sys.stderr)

    # â”€â”€ æ•°æ®é‡‡é›† â”€â”€
    all_raw = []

    if "scout" in sources:
        all_raw.extend(scan_scout())

    if "rednote" in sources:
        all_raw.extend(scan_rednote(kw_rednote))

    if "exa" in sources:
        all_raw.extend(scan_exa(kw_exa))

    if "x" in sources:
        all_raw.extend(scan_x(kw_x))

    if not all_raw:
        print("\nâŒ æ‰€æœ‰ä¿¡æºå‡æ— æ•°æ®", file=sys.stderr)
        sys.exit(1)

    print(f"\nğŸ“Š æ€»è®¡é‡‡é›† {len(all_raw)} æ¡æ•°æ®", file=sys.stderr)

    # â”€â”€ Gemini åˆ†æ â”€â”€
    analysis = None
    if not args.skip_analysis:
        analysis = analyze_with_gemini(all_raw)
        if analysis:
            print(f"\nâœ… åˆ†æå®Œæˆ", file=sys.stderr)
            # stdout JSON è¾“å‡º
            if args.json:
                print(json.dumps(analysis, ensure_ascii=False, indent=2))
        else:
            print(f"\nâš ï¸ åˆ†æå¤±è´¥ï¼Œä»…ä¿å­˜åŸå§‹æ•°æ®", file=sys.stderr)

    # â”€â”€ ä¿å­˜ â”€â”€
    save_outputs(all_raw, analysis, output_dir)

    print(f"\nâ•â•â• æ‰«æå®Œæˆ â•â•â•", file=sys.stderr)


if __name__ == "__main__":
    main()
