#!/usr/bin/env python3
"""
WeChat Writer - é«˜è´¨é‡å¾®ä¿¡å…¬ä¼—å·æ–‡ç« å†™ä½œpipeline

6é˜¶æ®µæµç¨‹ï¼šTOPICS â†’ PLAN â†’ RESEARCH â†’ WRITE â†’ REVIEW â†’ DE-AI
"""

import argparse
import json
import os
import sys
import re
import subprocess
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional

# Gemini SDK
try:
    from google import genai
    from google.genai import types
except ImportError:
    print("Error: google-genai not installed. Run: pip install google-genai", file=sys.stderr)
    sys.exit(1)

# ============================================================================
# Configuration
# ============================================================================

SCRIPT_DIR = Path(__file__).parent
PROMPTS_DIR = SCRIPT_DIR / "prompts"
PERSONAS_DIR = SCRIPT_DIR.parent / "personas"

# Model configuration
MODELS = {
    "topics": {"model": "gemini-3-flash-preview", "thinking": None},
    "plan": {"model": "gemini-3-flash-preview", "thinking": None},
    "research": {"model": "gemini-3-flash-preview", "thinking": None, "grounding": True},
    "write": {"model": "gemini-3-flash-preview", "thinking": None},
    "review": {"model": "gemini-3-flash-preview", "thinking": None},
    "deai": {"model": "gemini-3-flash-preview", "thinking": None},
}

# iCloud sync path
ICLOUD_ARTICLES_PATH = Path.home() / "Library/Mobile Documents/iCloud~md~obsidian/Documents/OpenClaw_Vault/Articles"

# ============================================================================
# Gemini Client
# ============================================================================

def get_client():
    """Get Gemini client."""
    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key:
        print("Error: GEMINI_API_KEY not set", file=sys.stderr)
        sys.exit(1)
    return genai.Client(api_key=api_key)


def call_gemini(
    prompt: str,
    stage: str,
    system_instruction: Optional[str] = None,
    max_retries: int = 3
) -> str:
    """Call Gemini API with stage-specific configuration."""
    client = get_client()
    config = MODELS[stage]
    model = config["model"]
    
    # Build generation config
    gen_config = {}
    if config.get("thinking"):
        gen_config["thinking_config"] = types.ThinkingConfig(
            thinkingBudget=10000 if config["thinking"] == "high" else 5000
        )
    
    # Build tools for grounding
    tools = None
    if config.get("grounding"):
        tools = [types.Tool(google_search=types.GoogleSearch())]
    
    for attempt in range(max_retries):
        try:
            response = client.models.generate_content(
                model=model,
                contents=prompt,
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction,
                    tools=tools,
                    **gen_config
                )
            )
            
            # Extract text from response
            text_parts = []
            for part in response.candidates[0].content.parts:
                if hasattr(part, 'text') and part.text:
                    text_parts.append(part.text)
            
            return "\n".join(text_parts)
            
        except Exception as e:
            if attempt < max_retries - 1:
                print(f"Retry {attempt + 1}/{max_retries}: {e}", file=sys.stderr)
                import time
                time.sleep(2 ** attempt)
            else:
                raise


def load_prompt(name: str) -> str:
    """Load prompt template."""
    prompt_file = PROMPTS_DIR / f"{name}.md"
    if prompt_file.exists():
        return prompt_file.read_text()
    return ""


def load_persona(name: str = "default") -> dict:
    """Load persona configuration."""
    persona_file = PERSONAS_DIR / f"{name}.yaml"
    if persona_file.exists():
        try:
            import yaml
            return yaml.safe_load(persona_file.read_text())
        except ImportError:
            # Fallback: return default persona without yaml
            return {
                "name": "æ€æ˜çš„AIè§‚å¯Ÿ",
                "tone": "åŠ¡å®ã€æœ‰æ€åº¦ã€å¶å°”è°ƒçš®",
                "avoid": ["é¦–å…ˆ", "å…¶æ¬¡", "å€¼å¾—æ³¨æ„çš„æ˜¯", "ç»¼ä¸Šæ‰€è¿°"],
                "prefer": ["ç›´æ¥ç»™ç»“è®º", "æ•¢ä¸‹åˆ¤æ–­", "é€‚å½“åæ§½"]
            }
    return {}


# ============================================================================
# Scout Scanner
# ============================================================================

def scan_scout_workspace(
    scout_workspace: Path,
    date_range_days: int = 3,
    topic_hint: Optional[str] = None
) -> dict:
    """Scan Scout workspace for relevant materials."""
    
    source_pack = {
        "scanned_at": datetime.now().isoformat(),
        "scout_workspace": str(scout_workspace),
        "date_range_days": date_range_days,
        "reports": [],
        "tweets": [],
        "videos": [],
        "articles": [],
        "notes": [],
    }
    
    cutoff_date = datetime.now() - timedelta(days=date_range_days)
    
    # Scan reports
    reports_dir = scout_workspace / "reports"
    if reports_dir.exists():
        for f in reports_dir.glob("*.md"):
            try:
                mtime = datetime.fromtimestamp(f.stat().st_mtime)
                if mtime >= cutoff_date:
                    content = f.read_text()
                    source_pack["reports"].append({
                        "path": str(f.relative_to(scout_workspace)),
                        "filename": f.name,
                        "modified": mtime.isoformat(),
                        "content": content[:5000],  # Truncate for context
                    })
            except Exception as e:
                print(f"Warning: Could not read {f}: {e}", file=sys.stderr)
    
    # Scan X posts
    x_dir = scout_workspace / "raw" / "x-posts"
    if x_dir.exists():
        for f in x_dir.glob("*.md"):
            try:
                mtime = datetime.fromtimestamp(f.stat().st_mtime)
                if mtime >= cutoff_date:
                    content = f.read_text()
                    source_pack["tweets"].append({
                        "path": str(f.relative_to(scout_workspace)),
                        "filename": f.name,
                        "modified": mtime.isoformat(),
                        "content": content[:8000],
                    })
            except Exception as e:
                print(f"Warning: Could not read {f}: {e}", file=sys.stderr)
    
    # Scan YouTube summaries
    yt_dir = scout_workspace / "raw" / "youtube"
    if yt_dir.exists():
        for f in yt_dir.rglob("*.md"):
            try:
                mtime = datetime.fromtimestamp(f.stat().st_mtime)
                if mtime >= cutoff_date:
                    content = f.read_text()
                    source_pack["videos"].append({
                        "path": str(f.relative_to(scout_workspace)),
                        "filename": f.name,
                        "modified": mtime.isoformat(),
                        "content": content[:5000],
                    })
            except Exception as e:
                print(f"Warning: Could not read {f}: {e}", file=sys.stderr)
    
    # Scan articles
    articles_dir = scout_workspace / "raw" / "articles"
    if articles_dir.exists():
        for f in articles_dir.glob("*.md"):
            try:
                mtime = datetime.fromtimestamp(f.stat().st_mtime)
                if mtime >= cutoff_date:
                    content = f.read_text()
                    source_pack["articles"].append({
                        "path": str(f.relative_to(scout_workspace)),
                        "filename": f.name,
                        "modified": mtime.isoformat(),
                        "content": content[:5000],
                    })
            except Exception as e:
                print(f"Warning: Could not read {f}: {e}", file=sys.stderr)
    
    # Scan notes
    notes_dir = scout_workspace / "notes"
    if notes_dir.exists():
        for f in notes_dir.glob("*.md"):
            try:
                mtime = datetime.fromtimestamp(f.stat().st_mtime)
                if mtime >= cutoff_date:
                    content = f.read_text()
                    source_pack["notes"].append({
                        "path": str(f.relative_to(scout_workspace)),
                        "filename": f.name,
                        "modified": mtime.isoformat(),
                        "content": content[:3000],
                    })
            except Exception as e:
                print(f"Warning: Could not read {f}: {e}", file=sys.stderr)
    
    return source_pack


# ============================================================================
# Pipeline Stages
# ============================================================================

def stage_topics(source_pack: dict, count: int = 5) -> dict:
    """Generate topic suggestions based on Scout materials."""
    
    # Always use inline template to avoid formatting issues
    prompt_template = None
    if not prompt_template:
        prompt_template = """åŸºäºä»¥ä¸‹Scoutæƒ…æŠ¥ï¼Œç”Ÿæˆ{count}ä¸ªå¾®ä¿¡å…¬ä¼—å·æ–‡ç« é€‰é¢˜å»ºè®®ã€‚

## Scoutæƒ…æŠ¥

### X/Twitterçƒ­ç‚¹
{tweets}

### è§†é¢‘æ‘˜è¦
{videos}

### åšå®¢æ–‡ç« 
{articles}

### æƒ…æŠ¥æŠ¥å‘Š
{reports}

## è¦æ±‚

1. æ¯ä¸ªé€‰é¢˜éœ€è¦æœ‰è¶³å¤Ÿçš„ç´ ææ”¯æ’‘
2. é€‰æ‹©æœ‰çƒ­åº¦ã€æœ‰äº‰è®®æˆ–æœ‰å®ç”¨ä»·å€¼çš„è¯é¢˜
3. ç»™å‡ºç‹¬ç‰¹çš„åˆ‡å…¥è§’åº¦ï¼Œä¸è¦æ³›æ³›è€Œè°ˆ

## è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰

è¾“å‡ºä¸€ä¸ªJSONå¯¹è±¡ï¼ŒåŒ…å«topicsæ•°ç»„ï¼Œæ¯ä¸ªtopicåŒ…å«ï¼šid, title, angle, heat, scout_support, key_sources, why_good
"""
    
    # Format source materials
    tweets_text = "\n\n".join([
        f"### {t['filename']}\n{t['content'][:2000]}"
        for t in source_pack.get("tweets", [])[:5]
    ]) or "æ— "
    
    videos_text = "\n\n".join([
        f"### {v['filename']}\n{v['content'][:1500]}"
        for v in source_pack.get("videos", [])[:5]
    ]) or "æ— "
    
    articles_text = "\n\n".join([
        f"### {a['filename']}\n{a['content'][:1500]}"
        for a in source_pack.get("articles", [])[:5]
    ]) or "æ— "
    
    reports_text = "\n\n".join([
        f"### {r['filename']}\n{r['content'][:2000]}"
        for r in source_pack.get("reports", [])[:3]
    ]) or "æ— "
    
    prompt = prompt_template.format(
        count=count,
        tweets=tweets_text,
        videos=videos_text,
        articles=articles_text,
        reports=reports_text,
    )
    
    print("ğŸ“‹ Generating topic suggestions...", file=sys.stderr)
    response = call_gemini(prompt, "topics")
    
    # Extract JSON from response
    json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
    if json_match:
        return json.loads(json_match.group(1))
    
    # Try parsing the whole response as JSON
    try:
        return json.loads(response)
    except json.JSONDecodeError:
        return {"topics": [], "raw_response": response}


def stage_plan(workdir: Path, topic: str) -> dict:
    """Generate article plan/outline."""
    
    source_pack_file = workdir / "source_pack.json"
    if not source_pack_file.exists():
        print("Error: source_pack.json not found. Run 'init' first.", file=sys.stderr)
        sys.exit(1)
    
    source_pack = json.loads(source_pack_file.read_text())
    
    # Always use inline template
    prompt_template = None
    if not prompt_template:
        prompt_template = """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å†…å®¹ç­–åˆ’ï¼Œä¸ºå¾®ä¿¡å…¬ä¼—å·ç­–åˆ’æ–‡ç« æ¡†æ¶ã€‚

## æ–‡ç« ä¸»é¢˜
{topic}

## å¯ç”¨ç´ æï¼ˆæ¥è‡ªScoutæƒ…æŠ¥ï¼‰
{source_summary}

## ä»»åŠ¡

ä¸ºè¿™ç¯‡æ–‡ç« åˆ¶å®šè¯¦ç»†çš„å†™ä½œè®¡åˆ’ï¼š
1. ç¡®å®šæ–‡ç« çš„æ ¸å¿ƒä¸»æ—¨å’Œç‹¬ç‰¹è§’åº¦
2. è®¾è®¡3-5ä¸ªä¸»è¦sections
3. ä¸ºæ¯ä¸ªsectionæ ‡æ³¨è¦ä½¿ç”¨çš„Scoutç´ æ
4. åˆ—å‡ºéœ€è¦é¢å¤–è°ƒç ”çš„é—®é¢˜

## è¾“å‡ºæ ¼å¼

è¾“å‡ºJSONå¯¹è±¡ï¼ŒåŒ…å«ï¼štitle, subtitle, core_message, angle, target_length, sectionsæ•°ç»„ï¼ˆæ¯ä¸ªsectionæœ‰heading, purpose, key_points, research_neededï¼‰, research_questionsæ•°ç»„
"""
    
    # Summarize sources
    source_summary = f"""
### X/Twitter ({len(source_pack.get('tweets', []))}æ¡)
{chr(10).join([t['filename'] for t in source_pack.get('tweets', [])[:10]])}

### è§†é¢‘æ‘˜è¦ ({len(source_pack.get('videos', []))}ä¸ª)
{chr(10).join([v['filename'] for v in source_pack.get('videos', [])[:10]])}

### åšå®¢æ–‡ç«  ({len(source_pack.get('articles', []))}ç¯‡)
{chr(10).join([a['filename'] for a in source_pack.get('articles', [])[:10]])}

### æƒ…æŠ¥æŠ¥å‘Š ({len(source_pack.get('reports', []))}ä»½)
{chr(10).join([r['filename'] for r in source_pack.get('reports', [])[:5]])}
"""
    
    prompt = prompt_template.format(
        topic=topic,
        source_summary=source_summary,
    )
    
    print("ğŸ“ Planning article structure...", file=sys.stderr)
    response = call_gemini(prompt, "plan")
    
    # Extract JSON
    json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
    if json_match:
        return json.loads(json_match.group(1))
    
    try:
        return json.loads(response)
    except json.JSONDecodeError:
        return {"raw_response": response}


def stage_research(workdir: Path) -> dict:
    """Conduct additional research based on plan."""
    
    plan_file = workdir / "plan.json"
    if not plan_file.exists():
        print("Error: plan.json not found. Run 'plan' first.", file=sys.stderr)
        sys.exit(1)
    
    plan = json.loads(plan_file.read_text())
    
    # Collect research questions
    questions = plan.get("research_questions", [])
    for section in plan.get("sections", []):
        questions.extend(section.get("research_needed", []))
    
    if not questions:
        print("âœ… No additional research needed.", file=sys.stderr)
        return {"research_items": [], "note": "No research needed"}
    
    prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç ”ç©¶å‘˜ã€‚è¯·é’ˆå¯¹ä»¥ä¸‹é—®é¢˜è¿›è¡Œè°ƒç ”ï¼Œä½¿ç”¨Google Searchè·å–æœ€æ–°ä¿¡æ¯ã€‚

## è°ƒç ”é—®é¢˜
{chr(10).join([f'{i+1}. {q}' for i, q in enumerate(questions)])}

## è¦æ±‚
1. æœç´¢æœ€æ–°ã€å¯é çš„ä¿¡æ¯æ¥æº
2. æå–å…³é”®æ•°æ®å’Œå¼•ç”¨
3. è®°å½•æ¥æºURL

## è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰

```json
{{
  "research_items": [
    {{
      "question": "é—®é¢˜",
      "findings": "è°ƒç ”ç»“æœ",
      "sources": [
        {{"title": "æ¥æºæ ‡é¢˜", "url": "URL"}}
      ],
      "key_quotes": ["å¯å¼•ç”¨çš„åŸæ–‡"]
    }}
  ]
}}
```
"""
    
    print("ğŸ” Researching...", file=sys.stderr)
    response = call_gemini(prompt, "research")
    
    # Extract JSON
    json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
    if json_match:
        return json.loads(json_match.group(1))
    
    try:
        return json.loads(response)
    except json.JSONDecodeError:
        return {"research_items": [], "raw_response": response}


def stage_write(workdir: Path) -> str:
    """Write the article draft."""
    
    # Load all materials
    source_pack = json.loads((workdir / "source_pack.json").read_text())
    plan = json.loads((workdir / "plan.json").read_text())
    
    research_file = workdir / "research.json"
    research = json.loads(research_file.read_text()) if research_file.exists() else {}
    
    prompt_template = load_prompt("write")
    if not prompt_template:
        prompt_template = """ä½ æ˜¯ä¸€ä½ä¼˜ç§€çš„ç§‘æŠ€å†…å®¹åˆ›ä½œè€…ï¼Œä¸ºå¾®ä¿¡å…¬ä¼—å·å†™æ–‡ç« ã€‚

## æ–‡ç« è®¡åˆ’
{plan}

## Scoutç´ æï¼ˆå¿…é¡»å¼•ç”¨ï¼‰
{sources}

## è¡¥å……è°ƒç ”
{research}

## å†™ä½œè¦æ±‚

1. **ä¸¥æ ¼æŒ‰ç…§plançš„sectionsç»“æ„å†™ä½œ**
2. **å¿…é¡»å¼•ç”¨Scoutç´ æä¸­çš„å†…å®¹**ï¼Œæ ¼å¼å¦‚ï¼šã€Œæ­£å¦‚@xxxæŒ‡å‡º...ã€ã€Œæ ¹æ®xxxæŠ¥é“...ã€
3. ç¯‡å¹…ï¼š{target_length}
4. å¼€å¤´è¦å¸å¼•äººï¼Œä¸è¦ã€Œéšç€AIçš„å‘å±•...ã€
5. ä¸­é—´æœ‰æ–™ï¼šæ•°æ®ã€æ¡ˆä¾‹ã€å¯¹æ¯”
6. ç»“å°¾æœ‰è§‚ç‚¹ï¼Œä¸æ˜¯æ€»ç»“

## è¾“å‡º

ç›´æ¥è¾“å‡ºMarkdownæ ¼å¼çš„æ–‡ç« ï¼Œä¸è¦åŒ…å«ä»£ç å—æ ‡è®°ã€‚
"""
    
    # Format sources for the prompt
    sources_text = ""
    for tweet in source_pack.get("tweets", [])[:3]:
        sources_text += f"\n### {tweet['filename']}\n{tweet['content'][:2000]}\n"
    for video in source_pack.get("videos", [])[:2]:
        sources_text += f"\n### {video['filename']}\n{video['content'][:1500]}\n"
    for article in source_pack.get("articles", [])[:2]:
        sources_text += f"\n### {article['filename']}\n{article['content'][:1500]}\n"
    
    prompt = prompt_template.format(
        plan=json.dumps(plan, ensure_ascii=False, indent=2),
        sources=sources_text or "æ— Scoutç´ æ",
        research=json.dumps(research, ensure_ascii=False, indent=2),
        target_length=plan.get("target_length", "2000-3000å­—"),
    )
    
    print("âœï¸ Writing draft...", file=sys.stderr)
    response = call_gemini(prompt, "write")
    
    return response


def stage_review(workdir: Path) -> dict:
    """Review the draft for quality."""
    
    draft_file = workdir / "draft.md"
    if not draft_file.exists():
        print("Error: draft.md not found. Run 'write' first.", file=sys.stderr)
        sys.exit(1)
    
    draft = draft_file.read_text()
    plan = json.loads((workdir / "plan.json").read_text())
    
    prompt = f"""ä½ æ˜¯ä¸€ä½ä¸¥æ ¼çš„å†…å®¹å®¡æ ¸ç¼–è¾‘ï¼Œè¯·å®¡æ ¸è¿™ç¯‡æ–‡ç« ã€‚

## æ–‡ç« è®¡åˆ’
{json.dumps(plan, ensure_ascii=False, indent=2)}

## æ–‡ç« è‰ç¨¿
{draft}

## å®¡æ ¸ç»´åº¦

1. **é€»è¾‘è¿è´¯æ€§**ï¼šæ®µè½ä¹‹é—´æ˜¯å¦æœ‰é€»è¾‘æ–­å±‚ï¼Ÿ
2. **äº‹å®å‡†ç¡®æ€§**ï¼šæœ‰æ²¡æœ‰æ˜æ˜¾çš„äº‹å®é”™è¯¯æˆ–å¤¸å¤§ï¼Ÿ
3. **ç»“æ„å®Œæ•´æ€§**ï¼šæ˜¯å¦è¦†ç›–äº†planä¸­çš„æ‰€æœ‰è¦ç‚¹ï¼Ÿ
4. **è§‚ç‚¹é²œæ˜åº¦**ï¼šæœ‰æ²¡æœ‰æ˜ç¡®çš„ä½œè€…è§‚ç‚¹ï¼Ÿ
5. **ç´ æå¼•ç”¨**ï¼šæ˜¯å¦æœ‰æ•ˆåˆ©ç”¨äº†Scoutç´ æï¼Ÿ

## è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰

```json
{{
  "overall_score": 8,
  "issues": [
    {{
      "severity": "high|medium|low",
      "location": "ç¬¬å‡ æ®µ/å“ªä¸ªsection",
      "issue": "é—®é¢˜æè¿°",
      "suggestion": "ä¿®æ”¹å»ºè®®"
    }}
  ],
  "strengths": ["äº®ç‚¹1", "äº®ç‚¹2"],
  "recommendation": "é€šè¿‡/éœ€è¦ä¿®æ”¹/é‡å†™"
}}
```
"""
    
    print("ğŸ” Reviewing draft...", file=sys.stderr)
    response = call_gemini(prompt, "review")
    
    # Extract JSON
    json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
    if json_match:
        return json.loads(json_match.group(1))
    
    try:
        return json.loads(response)
    except json.JSONDecodeError:
        return {"raw_response": response}


def stage_deai(workdir: Path, persona_name: str = "default") -> str:
    """Remove AI-ish writing style, make it authentic."""
    
    draft_file = workdir / "draft.md"
    if not draft_file.exists():
        print("Error: draft.md not found.", file=sys.stderr)
        sys.exit(1)
    
    draft = draft_file.read_text()
    
    review_file = workdir / "review.json"
    review = json.loads(review_file.read_text()) if review_file.exists() else {}
    
    persona = load_persona(persona_name)
    
    prompt_template = load_prompt("deai")
    if not prompt_template:
        prompt_template = """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å†…å®¹ç¼–è¾‘ï¼Œä¸“é—¨è´Ÿè´£è®©AIå†™çš„æ–‡ç« å˜å¾—æ›´æœ‰äººå‘³ã€‚

## åŸæ–‡
{draft}

## å®¡æ ¸åé¦ˆ
{review}

## å…¬ä¼—å·äººè®¾
{persona}

## å»AIåŒ–ä»»åŠ¡

AIæ–‡ç« çš„å…¸å‹é—®é¢˜ï¼š
- "é¦–å…ˆ...å…¶æ¬¡...æœ€å..." â†’ åˆ é™¤ï¼Œè‡ªç„¶è¿‡æ¸¡
- "å€¼å¾—æ³¨æ„çš„æ˜¯" â†’ åˆ é™¤ï¼Œæˆ–æ”¹æˆ"æœ‰ä¸ªå‘"
- "ç»¼ä¸Šæ‰€è¿°" â†’ ä¸éœ€è¦ï¼Œè¯»è€…ä¼šè‡ªå·±æ€»ç»“
- "è¿™æ˜¯ä¸€ä¸ªé‡è¦çš„é‡Œç¨‹ç¢‘" â†’ "è¿™äº‹å„¿æŒºå¤§çš„"
- æ¯æ®µéƒ½å¤ªå·¥æ•´ â†’ æœ‰äº›æ®µè½å¯ä»¥å¾ˆçŸ­ï¼Œå°±ä¸€å¥è¯
- æ²¡æœ‰ä¸ªäººè§‚ç‚¹ â†’ åŠ å…¥"æˆ‘è§‰å¾—..."ã€åæ§½ã€æ€åº¦

## å…·ä½“è¦æ±‚

1. åˆ é™¤æ‰€æœ‰è¿‡æ¸¡è¯å’Œå¥—è¯
2. åŠ å…¥ä½œè€…çš„ä¸»è§‚åˆ¤æ–­å’Œåæ§½
3. ç”¨å£è¯­åŒ–è¡¨è¾¾æ›¿æ¢ä¹¦é¢è¯­
4. è®©å¼€å¤´æ›´æŠ“äºº
5. ç»“å°¾è¦æœ‰æ€åº¦ï¼Œä¸æ˜¯æ€»ç»“
6. ä¿æŒæ ¸å¿ƒå†…å®¹ä¸å˜ï¼Œåªæ”¹è¡¨è¾¾æ–¹å¼

## è¾“å‡º

ç›´æ¥è¾“å‡ºä¿®æ”¹åçš„Markdownæ–‡ç« ï¼Œä¸è¦åŒ…å«ä»£ç å—æ ‡è®°ã€‚
"""
    
    prompt = prompt_template.format(
        draft=draft,
        review=json.dumps(review, ensure_ascii=False, indent=2),
        persona=json.dumps(persona, ensure_ascii=False, indent=2) if persona else "åŠ¡å®ã€æœ‰æ€åº¦ã€å¶å°”è°ƒçš®",
    )
    
    print("ğŸ¨ De-AI processing...", file=sys.stderr)
    response = call_gemini(prompt, "deai")
    
    return response


# ============================================================================
# Sync to iCloud
# ============================================================================

def sync_to_icloud(workdir: Path):
    """Sync article workdir to iCloud Obsidian."""
    
    target_dir = ICLOUD_ARTICLES_PATH / workdir.name
    target_dir.mkdir(parents=True, exist_ok=True)
    
    # Sync files
    subprocess.run([
        "rsync", "-av", "--delete",
        f"{workdir}/",
        f"{target_dir}/"
    ], check=True)
    
    print(f"â˜ï¸ Synced to: {target_dir}", file=sys.stderr)
    return target_dir


# ============================================================================
# Tone Adjust Stage
# ============================================================================

def stage_tone_adjust(workdir: Path, instruction: str) -> str:
    """
    æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤å¯¹ final.md è¿›è¡Œæ”¹å†™è°ƒæ•´ï¼ˆè¯­æ°”/é£æ ¼/æªè¾ç­‰ï¼‰ã€‚
    å…¨ç¨‹æ–‡ä»¶äº¤æ¢ï¼Œä¸ä¼ å¤§æ–‡æœ¬ç»™ shellã€‚
    """
    import tempfile

    final_path = workdir / "final.md"
    if not final_path.exists():
        raise FileNotFoundError(f"final.md ä¸å­˜åœ¨: {final_path}ï¼Œè¯·å…ˆå®Œæˆ deai é˜¶æ®µ")

    print("ğŸ¨ Adjusting tone/style...", file=sys.stderr)

    # è¯»å–æ–‡ç« å†…å®¹
    article = final_path.read_text(encoding="utf-8")

    # æ„å»º promptï¼ˆå†™åˆ°ä¸´æ—¶æ–‡ä»¶ï¼Œé¿å… shell å˜é‡ä¼ å¤§æ–‡æœ¬ï¼‰
    prompt = f"""è¯·æŒ‰ç…§ä»¥ä¸‹æŒ‡ä»¤å¯¹è¿™ç¯‡å¾®ä¿¡å…¬ä¼—å·æ–‡ç« è¿›è¡Œæ”¹å†™ï¼š

ã€æ”¹å†™æŒ‡ä»¤ã€‘
{instruction}

ã€è¦æ±‚ã€‘
- ä¸¥æ ¼æŒ‰ç…§æŒ‡ä»¤æ”¹å†™ï¼Œè§‚ç‚¹å’Œå†…å®¹ç»“æ„ä¿æŒä¸å˜
- ä¿ç•™æ‰€æœ‰å…·ä½“æ•°æ®ã€æ¡ˆä¾‹å’Œå¼•ç”¨
- ä¿ç•™åŸæœ‰æ ‡é¢˜å’Œç« èŠ‚ç»“æ„ï¼ˆ## æ ¼å¼ï¼‰
- ç›´æ¥è¾“å‡ºæ”¹å†™åçš„å®Œæ•´æ–‡ç« ï¼Œä¸è¦è§£é‡Šæˆ–æ€»ç»“

ã€åŸæ–‡ã€‘
{article}"""

    # å†™ prompt åˆ°ä¸´æ—¶æ–‡ä»¶
    with tempfile.NamedTemporaryFile(mode="w", suffix=".txt", delete=False, encoding="utf-8") as f:
        f.write(prompt)
        prompt_file = f.name

    try:
        client = get_client()
        with open(prompt_file, "r", encoding="utf-8") as f:
            full_prompt = f.read()

        response = client.models.generate_content(
            model=MODELS["deai"]["model"],
            contents=full_prompt,
        )
        result = response.text.strip()
    finally:
        Path(prompt_file).unlink(missing_ok=True)

    # å¤‡ä»½åŸæ–‡
    backup_path = workdir / "final_before_tone_adjust.md"
    backup_path.write_text(article, encoding="utf-8")

    # å†™å› final.md
    final_path.write_text(result, encoding="utf-8")

    print(f"âœ… tone-adjust completed (backup: {backup_path.name})", file=sys.stderr)
    return result


def cmd_tone_adjust(args):
    workdir = Path(args.workdir)
    if not workdir.exists():
        print(f"é”™è¯¯: workdir ä¸å­˜åœ¨: {workdir}", file=sys.stderr)
        sys.exit(1)

    stage_tone_adjust(workdir, args.instruction)

    if args.sync:
        sync_to_icloud(workdir)


# ============================================================================
# CLI Commands
# ============================================================================

def cmd_init(args):
    """Initialize article and generate topics."""
    
    scout_workspace = Path(args.scout_workspace).expanduser()
    output_dir = Path(args.output).expanduser()
    
    # Parse date range
    date_range_days = 3
    if args.date_range:
        if args.date_range.endswith('d'):
            date_range_days = int(args.date_range[:-1])
        else:
            date_range_days = int(args.date_range)
    
    # Create output directory
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Scan Scout workspace
    print(f"ğŸ“¡ Scanning Scout workspace: {scout_workspace}", file=sys.stderr)
    source_pack = scan_scout_workspace(scout_workspace, date_range_days)
    
    # Save source pack
    source_pack_file = output_dir / "source_pack.json"
    source_pack_file.write_text(json.dumps(source_pack, ensure_ascii=False, indent=2))
    print(f"âœ… Saved: {source_pack_file}", file=sys.stderr)
    
    # Generate topics
    topics = stage_topics(source_pack, count=args.count or 5)
    
    # Save topics
    topics_file = output_dir / "topics.json"
    topics_file.write_text(json.dumps(topics, ensure_ascii=False, indent=2))
    print(f"âœ… Saved: {topics_file}", file=sys.stderr)
    
    # Print topics
    print("\n" + "="*60)
    print("ğŸ“‹ é€‰é¢˜å»ºè®®")
    print("="*60)
    for topic in topics.get("topics", []):
        print(f"\n{topic.get('id', '?')}. {topic.get('title', 'Unknown')}")
        print(f"   è§’åº¦: {topic.get('angle', 'N/A')}")
        print(f"   çƒ­åº¦: {topic.get('heat', 'N/A')}")
        scout_support = topic.get('scout_support', {})
        if isinstance(scout_support, str):
            scout_support = {}
        print(f"   ç´ æ: tweets={scout_support.get('tweets', 0)}, "
              f"videos={scout_support.get('videos', 0)}, "
              f"articles={scout_support.get('articles', 0)}")
        print(f"   ç†ç”±: {topic.get('why_good', 'N/A')}")


def cmd_run(args):
    """Run full pipeline for selected topic."""
    
    workdir = Path(args.workdir).expanduser()
    topic = args.topic
    
    if not workdir.exists():
        print(f"Error: Workdir not found: {workdir}", file=sys.stderr)
        sys.exit(1)
    
    # PLAN
    print("\n" + "="*60)
    print("ğŸ“ Stage 1: PLAN")
    print("="*60)
    plan = stage_plan(workdir, topic)
    plan_file = workdir / "plan.json"
    plan_file.write_text(json.dumps(plan, ensure_ascii=False, indent=2))
    print(f"âœ… Saved: {plan_file}")
    
    # RESEARCH
    print("\n" + "="*60)
    print("ğŸ” Stage 2: RESEARCH")
    print("="*60)
    research = stage_research(workdir)
    research_file = workdir / "research.json"
    research_file.write_text(json.dumps(research, ensure_ascii=False, indent=2))
    print(f"âœ… Saved: {research_file}")
    
    # WRITE
    print("\n" + "="*60)
    print("âœï¸ Stage 3: WRITE")
    print("="*60)
    draft = stage_write(workdir)
    draft_file = workdir / "draft.md"
    draft_file.write_text(draft)
    print(f"âœ… Saved: {draft_file}")
    
    # REVIEW
    print("\n" + "="*60)
    print("ğŸ” Stage 4: REVIEW")
    print("="*60)
    review = stage_review(workdir)
    review_file = workdir / "review.json"
    review_file.write_text(json.dumps(review, ensure_ascii=False, indent=2))
    print(f"âœ… Saved: {review_file}")
    print(f"   Score: {review.get('overall_score', 'N/A')}/10")
    print(f"   Recommendation: {review.get('recommendation', 'N/A')}")
    
    # DE-AI
    print("\n" + "="*60)
    print("ğŸ¨ Stage 5: DE-AI")
    print("="*60)
    final = stage_deai(workdir, args.persona or "default")
    final_file = workdir / "final.md"
    final_file.write_text(final)
    print(f"âœ… Saved: {final_file}")
    
    # Sync to iCloud
    print("\n" + "="*60)
    print("â˜ï¸ Syncing to iCloud Obsidian")
    print("="*60)
    icloud_path = sync_to_icloud(workdir)
    
    print("\n" + "="*60)
    print("ğŸ‰ DONE!")
    print("="*60)
    print(f"\nğŸ“ Local: {workdir}")
    print(f"â˜ï¸ iCloud: {icloud_path}")
    print(f"\nğŸ‘‰ Review at: {icloud_path / 'final.md'}")


def cmd_single_stage(args, stage_name: str):
    """Run a single stage."""
    workdir = Path(args.workdir).expanduser()
    
    if stage_name == "plan":
        result = stage_plan(workdir, args.topic)
        (workdir / "plan.json").write_text(json.dumps(result, ensure_ascii=False, indent=2))
    elif stage_name == "research":
        result = stage_research(workdir)
        (workdir / "research.json").write_text(json.dumps(result, ensure_ascii=False, indent=2))
    elif stage_name == "write":
        result = stage_write(workdir)
        (workdir / "draft.md").write_text(result)
    elif stage_name == "review":
        result = stage_review(workdir)
        (workdir / "review.json").write_text(json.dumps(result, ensure_ascii=False, indent=2))
    elif stage_name == "deai":
        result = stage_deai(workdir, getattr(args, 'persona', 'default'))
        (workdir / "final.md").write_text(result)
    
    print(f"âœ… {stage_name} completed")


# ============================================================================
# Main
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="WeChat Writer - é«˜è´¨é‡å…¬ä¼—å·æ–‡ç« å†™ä½œpipeline"
    )
    subparsers = parser.add_subparsers(dest="command", required=True)
    
    # init command
    init_parser = subparsers.add_parser("init", help="åˆå§‹åŒ–æ–‡ç« ï¼Œæ‰«æScoutç”Ÿæˆé€‰é¢˜")
    init_parser.add_argument("--scout-workspace", required=True, help="Scout workspaceè·¯å¾„")
    init_parser.add_argument("--date-range", default="3d", help="æ‰«ææ—¥æœŸèŒƒå›´ï¼Œå¦‚3d")
    init_parser.add_argument("--count", type=int, default=5, help="ç”Ÿæˆé€‰é¢˜æ•°é‡")
    init_parser.add_argument("--output", required=True, help="è¾“å‡ºç›®å½•")
    
    # run command
    run_parser = subparsers.add_parser("run", help="æ‰§è¡Œå®Œæ•´å†™ä½œpipeline")
    run_parser.add_argument("--workdir", required=True, help="æ–‡ç« å·¥ä½œç›®å½•")
    run_parser.add_argument("--topic", required=True, help="é€‰å®šçš„æ–‡ç« ä¸»é¢˜")
    run_parser.add_argument("--persona", default="default", help="å…¬ä¼—å·äººè®¾é…ç½®")
    
    # Single stage commands
    for stage in ["plan", "research", "write", "review", "deai"]:
        stage_parser = subparsers.add_parser(stage, help=f"å•ç‹¬æ‰§è¡Œ{stage}é˜¶æ®µ")
        stage_parser.add_argument("--workdir", required=True, help="æ–‡ç« å·¥ä½œç›®å½•")
        if stage == "plan":
            stage_parser.add_argument("--topic", required=True, help="æ–‡ç« ä¸»é¢˜")
        if stage == "deai":
            stage_parser.add_argument("--persona", default="default", help="äººè®¾é…ç½®")

    # tone-adjust command
    tone_parser = subparsers.add_parser("tone-adjust", help="å¯¹final.mdè¿›è¡Œè¯­æ°”/é£æ ¼è°ƒæ•´")
    tone_parser.add_argument("--workdir", required=True, help="æ–‡ç« å·¥ä½œç›®å½•")
    tone_parser.add_argument("--instruction", required=True, help="æ”¹å†™æŒ‡ä»¤ï¼Œå¦‚'è¯­æ°”æŸ”å’Œä¸€ç‚¹ï¼Œä¸è¦å¤ªåˆ»è–„'")
    tone_parser.add_argument("--sync", action="store_true", help="è°ƒæ•´ååŒæ­¥åˆ°iCloud Obsidian")

    args = parser.parse_args()

    if args.command == "init":
        cmd_init(args)
    elif args.command == "run":
        cmd_run(args)
    elif args.command == "tone-adjust":
        cmd_tone_adjust(args)
    else:
        cmd_single_stage(args, args.command)


if __name__ == "__main__":
    main()
