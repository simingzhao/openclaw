#!/usr/bin/env python3
"""å°çº¢ä¹¦å†…å®¹ç²¾ä¿®å™¨ï¼ˆGemini å¤šç±»å‹è·¯ç”±ï¼‰ã€‚"""

import argparse
import json
import os
import sys
from datetime import datetime

try:
    from google import genai
    from google.genai import types
except ImportError:
    print("éœ€è¦ google-genai: pip install google-genai", file=sys.stderr)
    sys.exit(1)

MODEL_PRIMARY = os.environ.get("REDNOTE_MODEL", "gemini-3.1-pro-preview")
MODEL_FALLBACK = "gemini-3-flash-preview"

CONTENT_TYPES = ["brief", "analysis", "opinion", "tools"]

SYSTEM_PROMPTS = {
    "brief": """ä½ æ˜¯ä¸€ä¸ªå°çº¢ä¹¦AIæ—¥æŠ¥çš„å†…å®¹ç¼–è¾‘ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä»AIé¢†åŸŸå·¡é€»æ‘˜è¦ä¸­ï¼ŒæŒ‘é€‰æœ€æœ‰ä»·å€¼çš„æ–°é—»ï¼Œç²¾ä¿®æˆå°çº¢ä¹¦é£æ ¼å¡ç‰‡æ–‡æ¡ˆã€‚

ä¸¥æ ¼è¾“å‡º JSONï¼Œä¸è¦è¾“å‡ºä»»ä½•é¢å¤–æ–‡æœ¬ã€‚æ ¼å¼ï¼š
{
  "cover_title": "AIæ—¥æŠ¥ MM.DD",
  "cover_subtitle": "ä¸€å¥è¯é’©å­ï¼ˆ<=10å­—ï¼‰",
  "post_title": "å¸–å­æ ‡é¢˜ï¼ˆ<=20å­—ï¼‰",
  "post_body": "å¸–å­æ­£æ–‡ï¼ˆ<=800å­—ï¼Œä¸å«tagsï¼‰",
  "tags": ["AI", "äººå·¥æ™ºèƒ½", "ç§‘æŠ€"],
  "items": [
    {
      "title": "å¡ç‰‡æ ‡é¢˜ï¼ˆ<=12å­—ï¼‰",
      "body": "æ ¸å¿ƒè§‚ç‚¹è¡Œ1\\næ ¸å¿ƒè§‚ç‚¹è¡Œ2\\n---\\nå…³é”®æ•°æ®è¡Œ1\\n---\\né‡‘å¥è¡Œ1"
    }
  ]
}

è¦æ±‚ï¼š
1. åªé€‰5-7æ¡æœ€æœ‰ä»·å€¼ä¿¡æ¯ã€‚
2. items[].body ä½¿ç”¨ --- åˆ†æˆ 3 æ®µï¼ˆæ ¸å¿ƒè§‚ç‚¹/å…³é”®æ•°æ®/é‡‘å¥ï¼‰ã€‚
3. æ¯è¡ŒçŸ­å¥ï¼Œå£è¯­åŒ–ã€æœ‰æ€åº¦ã€‚
4. tags 3-5ä¸ªä¸”å¿…é¡»åŒ…å« AIã€‚""",
    "analysis": """ä½ æ˜¯å°çº¢ä¹¦æ·±åº¦åˆ†æå†™æ‰‹ã€‚ä½ éœ€è¦å›´ç»•ä¸€ä¸ªæœ€å€¼å¾—è®¨è®ºçš„ä¸»é¢˜åšæ·±æŒ–ã€‚

ä¸¥æ ¼è¾“å‡º JSONï¼Œä¸è¦è¾“å‡ºä»»ä½•é¢å¤–æ–‡æœ¬ã€‚æ ¼å¼ï¼š
{
  "title": "åˆ†ææ ‡é¢˜ï¼ˆ<=24å­—ï¼‰",
  "key_quote": "æ ¸å¿ƒé‡‘å¥ï¼ˆ1å¥ï¼‰",
  "sections": [
    {
      "heading": "å°èŠ‚æ ‡é¢˜",
      "points": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"],
      "quote": "è¯¥å°èŠ‚é‡‘å¥"
    }
  ],
  "post_body": "å®Œæ•´æ­£æ–‡ï¼ˆ<=1200å­—ï¼‰",
  "tags": ["AI", "äººå·¥æ™ºèƒ½", "è¶‹åŠ¿"]
}

è¦æ±‚ï¼š
1. åªåšä¸€ä¸ªä¸»é¢˜æ·±æŒ–ï¼Œä¸åšæ–°é—»æ‹¼ç›˜ã€‚
2. sections å»ºè®® 3-5 ä¸ªï¼Œæ¯èŠ‚ points 3-5 æ¡ã€‚
3. ç”¨äº‹å®ã€æ•°æ®ã€è§‚ç‚¹å¹¶é‡ï¼›è¯­è¨€é€‚åˆå°çº¢ä¹¦é˜…è¯»ã€‚
4. post_body è¦æœ‰ç»“è®ºæ€§åˆ¤æ–­ã€‚""",
    "opinion": """ä½ æ˜¯å°çº¢ä¹¦çƒ­ç‚¹è¯„è®ºå†™æ‰‹ã€‚è¯·è¾“å‡ºçŸ­ã€ç‹ ã€æ¸…æ™°çš„è§‚ç‚¹æ–‡æ¡ˆã€‚

ä¸¥æ ¼è¾“å‡º JSONï¼Œä¸è¦è¾“å‡ºä»»ä½•é¢å¤–æ–‡æœ¬ã€‚æ ¼å¼ï¼š
{
  "title": "è§‚ç‚¹æ ‡é¢˜ï¼ˆ<=20å­—ï¼‰",
  "body": "çŸ­è¯„æ­£æ–‡ï¼ˆ150-350å­—ï¼Œé”‹åˆ©ä½†ä¸è¿‡æ¿€ï¼‰",
  "tags": ["AI", "è§‚ç‚¹", "ç§‘æŠ€"]
}

è¦æ±‚ï¼š
1. åªè¡¨è¾¾ä¸€ä¸ªæ ¸å¿ƒåˆ¤æ–­ã€‚
2. æ–‡é£æœ‰æ€åº¦ï¼Œé¿å…ç©ºè¯å¥—è¯ã€‚
3. ä¸ç”Ÿæˆ itemsï¼Œä¸ç”Ÿæˆ sectionsã€‚""",
    "tools": """ä½ æ˜¯å°çº¢ä¹¦å·¥å…·è¯„æµ‹å†™æ‰‹ã€‚è¯·è¾“å‡ºå¯ç›´æ¥å‘å¸ƒçš„å·¥å…·æ¨èæ¸…å•ã€‚

ä¸¥æ ¼è¾“å‡º JSONï¼Œä¸è¦è¾“å‡ºä»»ä½•é¢å¤–æ–‡æœ¬ã€‚æ ¼å¼ï¼š
{
  "title": "å·¥å…·æ¸…å•æ ‡é¢˜ï¼ˆ<=24å­—ï¼‰",
  "tools": [
    {
      "name": "å·¥å…·å",
      "description": "æ ¸å¿ƒèƒ½åŠ›ä¸é€‚ç”¨åœºæ™¯ï¼ˆ1-2å¥ï¼‰",
      "verdict": "ä¸€å¥ç»“è®ºï¼ˆå€¼ä¸å€¼å¾—ç”¨ï¼‰"
    }
  ],
  "post_body": "æ­£æ–‡ï¼ˆ<=900å­—ï¼‰",
  "tags": ["AI", "å·¥å…·æ¨è", "æ•ˆç‡"]
}

è¦æ±‚ï¼š
1. tools æ•°é‡ 4-8 ä¸ªã€‚
2. ç»“è®ºå¿…é¡»æ˜ç¡®ï¼Œä¸ä¸­åº¸ã€‚
3. å†™å‡ºä½¿ç”¨é—¨æ§›å’Œé€‚åˆäººç¾¤ã€‚""",
}


def _build_user_prompt(digest_text: str, date_str: str, content_type: str) -> str:
    guidance = {
        "brief": "è¯·ä»ç´ æä¸­æŒ‘é€‰5-7æ¡æœ€æœ‰ä»·å€¼æ–°é—»ï¼Œäº§å‡ºæ—¥æŠ¥å¡ç‰‡å†…å®¹ã€‚",
        "analysis": "è¯·æç‚¼ä¸€ä¸ªå€¼å¾—æ·±æŒ–çš„ä¸»é¢˜å¹¶å®Œæˆç»“æ„åŒ–åˆ†æã€‚",
        "opinion": "è¯·é’ˆå¯¹æœ€æœ‰äº‰è®®çš„è¯é¢˜å†™ä¸€æ®µçŸ­è¯„ã€‚",
        "tools": "è¯·æŒ‘é€‰æœ€å€¼å¾—æ¨èçš„å·¥å…·å¹¶ç»™å‡ºæ˜ç¡®ç»“è®ºã€‚",
    }
    return (
        f"ä»Šå¤©æ—¥æœŸï¼š{date_str}\n"
        f"å†…å®¹ç±»å‹ï¼š{content_type}\n"
        f"ä»»åŠ¡ï¼š{guidance[content_type]}\n\n"
        "ç´ æå¦‚ä¸‹ï¼š\n"
        "---\n"
        f"{digest_text}\n"
        "---\n\n"
        "ä¸¥æ ¼æŒ‰ç…§ç³»ç»Ÿæç¤ºè¦æ±‚è¾“å‡º JSONã€‚ç¡®ä¿ JSON å®Œæ•´å¯è§£æï¼Œä¸è¦æˆªæ–­ã€‚"
    )


def _extract_json(text: str) -> str:
    data = text.strip()
    if "```json" in data:
        data = data.split("```json", 1)[1]
        data = data.split("```", 1)[0]
    elif "```" in data:
        data = data.split("```", 1)[1]
        data = data.split("```", 1)[0]
    return data.strip()


def _validate_output(data: dict, content_type: str) -> None:
    required_by_type = {
        "brief": ["items", "post_body"],
        "analysis": ["title", "sections", "key_quote", "post_body"],
        "opinion": ["title", "body", "tags"],
        "tools": ["title", "tools", "post_body", "tags"],
    }

    missing = [k for k in required_by_type[content_type] if k not in data]
    if missing:
        print(f"âŒ è¾“å‡ºç¼ºå°‘å­—æ®µ: {', '.join(missing)}", file=sys.stderr)
        sys.exit(1)

    if content_type == "brief":
        if not isinstance(data.get("items"), list) or not data["items"]:
            print("âŒ brief è¾“å‡º items ä¸ºç©º", file=sys.stderr)
            sys.exit(1)
    elif content_type == "analysis":
        if not isinstance(data.get("sections"), list) or not data["sections"]:
            print("âŒ analysis è¾“å‡º sections ä¸ºç©º", file=sys.stderr)
            sys.exit(1)
    elif content_type == "tools":
        if not isinstance(data.get("tools"), list) or not data["tools"]:
            print("âŒ tools è¾“å‡º tools ä¸ºç©º", file=sys.stderr)
            sys.exit(1)


def generate_content(
    digest_text: str,
    date_str: str | None = None,
    content_type: str = "brief",
) -> dict:
    """è°ƒç”¨ Gemini è¿›è¡Œå¤šç±»å‹å†…å®¹ç”Ÿæˆã€‚"""
    if content_type not in CONTENT_TYPES:
        raise ValueError(f"Unsupported content_type: {content_type}")

    if not date_str:
        date_str = datetime.now().strftime("%m.%d")

    client = genai.Client()
    user_prompt = _build_user_prompt(digest_text, date_str, content_type)
    system_prompt = SYSTEM_PROMPTS[content_type]

    model = MODEL_PRIMARY
    response = None

    for attempt_model in [MODEL_PRIMARY, MODEL_FALLBACK]:
        try:
            print(f"ğŸ¤– å°è¯•æ¨¡å‹: {attempt_model}")
            response = client.models.generate_content(
                model=attempt_model,
                contents=user_prompt,
                config=types.GenerateContentConfig(
                    system_instruction=system_prompt,
                    temperature=0.7,
                    max_output_tokens=8192,
                ),
            )
            model = attempt_model
            break
        except Exception as e:
            if "503" in str(e) or "UNAVAILABLE" in str(e):
                print(f"âš ï¸ {attempt_model} ä¸å¯ç”¨ (503)ï¼Œå°è¯• fallback...")
                continue
            raise
    else:
        print("âŒ æ‰€æœ‰æ¨¡å‹éƒ½ä¸å¯ç”¨", file=sys.stderr)
        sys.exit(1)

    if response is None or not response.text:
        print("âŒ æ¨¡å‹è¿”å›ä¸ºç©º", file=sys.stderr)
        sys.exit(1)

    print(f"âœ… ä½¿ç”¨æ¨¡å‹: {model}")

    raw = _extract_json(response.text)
    try:
        data = json.loads(raw)
    except json.JSONDecodeError as e:
        print(f"âŒ JSONè§£æå¤±è´¥: {e}", file=sys.stderr)
        print(f"åŸå§‹è¾“å‡ºç‰‡æ®µ:\n{response.text[:500]}", file=sys.stderr)
        sys.exit(1)

    _validate_output(data, content_type)

    if content_type == "brief":
        print(f"âœ… ç”Ÿæˆ brief å†…å®¹: {len(data.get('items', []))} æ¡å¡ç‰‡")
    elif content_type == "analysis":
        print(f"âœ… ç”Ÿæˆ analysis å†…å®¹: {len(data.get('sections', []))} ä¸ªç« èŠ‚")
    elif content_type == "tools":
        print(f"âœ… ç”Ÿæˆ tools å†…å®¹: {len(data.get('tools', []))} ä¸ªå·¥å…·")
    else:
        print("âœ… ç”Ÿæˆ opinion å†…å®¹")

    return data


def load_digest(path: str) -> str:
    """è¯»å– digest æ–‡ä»¶ã€‚"""
    with open(path, "r", encoding="utf-8") as f:
        return f.read()


def load_latest_digest(workspace: str, source: str = "x") -> tuple[str, str]:
    """ä» workspace åŠ è½½æœ€æ–° digestã€‚"""
    today = datetime.now().strftime("%Y-%m-%d")
    date_display = datetime.now().strftime("%m.%d")
    texts = []

    if source in ("x", "both"):
        x_path = os.path.join(workspace, "raw", "x-posts", f"{today}_digest.md")
        if os.path.exists(x_path):
            texts.append(f"## X/Twitter å·¡é€»\n\n{load_digest(x_path)}")
        else:
            print(f"âš ï¸ X digest ä¸å­˜åœ¨: {x_path}", file=sys.stderr)

    if source in ("youtube", "both"):
        yt_dir = os.path.join(workspace, "raw", "youtube")
        if os.path.isdir(yt_dir):
            yt_texts = []
            for channel in sorted(os.listdir(yt_dir)):
                sum_dir = os.path.join(yt_dir, channel, "summaries")
                if not os.path.isdir(sum_dir):
                    continue
                for filename in sorted(os.listdir(sum_dir)):
                    if filename.startswith(today) and filename.endswith(".md"):
                        yt_texts.append(load_digest(os.path.join(sum_dir, filename)))
            if yt_texts:
                texts.append("## YouTube å·¡é€»\n\n" + "\n\n---\n\n".join(yt_texts))

    if not texts:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»Šå¤©çš„å·¡é€»ç´ æ", file=sys.stderr)
        sys.exit(1)

    combined = "\n\n".join(texts)
    if len(combined) > 30000:
        print(f"âš ï¸ ç´ æå¤ªé•¿ï¼ˆ{len(combined)}å­—ï¼‰ï¼Œæˆªæ–­è‡³30000å­—")
        combined = combined[:30000]

    return combined, date_display


def main() -> None:
    parser = argparse.ArgumentParser(description="å°çº¢ä¹¦å†…å®¹ç²¾ä¿®å™¨ï¼ˆGemini å¤šç±»å‹ï¼‰")
    sub = parser.add_subparsers(dest="command")

    p_file = sub.add_parser("from-file", help="ä»æŒ‡å®š digest æ–‡ä»¶ç”Ÿæˆ")
    p_file.add_argument("--input", "-i", required=True, help="digest æ–‡ä»¶è·¯å¾„")
    p_file.add_argument("--date", "-d", help="æ—¥æœŸï¼ˆMM.DDæ ¼å¼ï¼‰")
    p_file.add_argument("--type", choices=CONTENT_TYPES, default="brief", help="å†…å®¹ç±»å‹")
    p_file.add_argument("--output", "-o", help="è¾“å‡º JSON è·¯å¾„")

    p_auto = sub.add_parser("auto", help="è‡ªåŠ¨åŠ è½½æœ€æ–° digest ç”Ÿæˆ")
    p_auto.add_argument("--workspace", "-w", default=os.path.expanduser("~/.openclaw/workspace"))
    p_auto.add_argument("--source", "-s", choices=["x", "youtube", "both"], default="both")
    p_auto.add_argument("--date", "-d", help="æ—¥æœŸï¼ˆMM.DDæ ¼å¼ï¼‰")
    p_auto.add_argument("--type", choices=CONTENT_TYPES, default="brief", help="å†…å®¹ç±»å‹")
    p_auto.add_argument("--output", "-o", help="è¾“å‡º JSON è·¯å¾„")

    args = parser.parse_args()
    if not args.command:
        parser.print_help()
        sys.exit(1)

    if args.command == "from-file":
        digest_text = load_digest(args.input)
        date_str = args.date or datetime.now().strftime("%m.%d")
    else:
        digest_text, date_str = load_latest_digest(args.workspace, args.source)
        if args.date:
            date_str = args.date

    print(f"ğŸ“ ç´ æé•¿åº¦: {len(digest_text)}å­—")
    print(f"ğŸ“… æ—¥æœŸ: {date_str}")
    print(f"ğŸ§© ç±»å‹: {args.type}")
    print(f"ğŸ¤– æ¨¡å‹: {MODEL_PRIMARY} (fallback: {MODEL_FALLBACK})")
    print("â³ è°ƒç”¨ Gemini ç²¾ä¿®ä¸­...\n")

    data = generate_content(digest_text, date_str, content_type=args.type)
    output_json = json.dumps(data, ensure_ascii=False, indent=2)

    if args.output:
        os.makedirs(os.path.dirname(args.output) or ".", exist_ok=True)
        with open(args.output, "w", encoding="utf-8") as f:
            f.write(output_json)
        print(f"\nâœ… å·²ä¿å­˜: {args.output}")
    else:
        print(f"\n{output_json}")


if __name__ == "__main__":
    main()
